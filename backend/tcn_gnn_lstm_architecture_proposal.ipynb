{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN-GNN-LSTM Hybrid Architecture\n",
    "\n",
    "## A Complete Deep Learning Framework for Multi-Asset Crypto Portfolio Optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "This document proposes and implements a **novel hybrid architecture** that combines three powerful deep learning paradigms:\n",
    "\n",
    "1. **Temporal Convolutional Networks (TCN)** - For multi-scale temporal feature extraction\n",
    "2. **Graph Neural Networks (GNN)** - For modeling dynamic cross-asset relationships\n",
    "3. **Long Short-Term Memory (LSTM)** - For sequential prediction with memory\n",
    "\n",
    "The architecture features a **multi-head output** system for:\n",
    "- **Trading Head**: Direct portfolio weight predictions\n",
    "- **Prediction Head**: Gaussian distribution (mean + uncertainty)\n",
    "- **Value Head**: RL-compatible value function estimation\n",
    "\n",
    "---\n",
    "\n",
    "**Document Version:** 2.0 (Theory + Implementation)  \n",
    "**Author:** AI Trading Research Team  \n",
    "**Date:** February 2026  \n",
    "**Status:** Complete Implementation with Theoretical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Part I: Theoretical Foundation\n",
    "1. [Motivation & Current Limitations](#1-motivation)\n",
    "2. [Architecture Overview](#2-architecture)\n",
    "3. [Component Deep Dive](#3-components)\n",
    "   - 3.1 TCN Feature Extractor\n",
    "   - 3.2 Dynamic GNN for Asset Correlations\n",
    "   - 3.3 LSTM Sequential Processor\n",
    "   - 3.4 Multi-Head Output System\n",
    "4. [Mathematical Formulation](#4-math)\n",
    "5. [Training Strategy - Curriculum Learning](#5-training-theory)\n",
    "6. [Uncertainty-Aware Prediction](#6-uncertainty-theory)\n",
    "7. [Loss Function Design](#7-loss-theory)\n",
    "\n",
    "### Part II: Implementation\n",
    "8. [Environment Setup](#8-setup)\n",
    "9. [Data Loading Pipeline](#9-data)\n",
    "10. [Exploratory Data Analysis](#10-eda)\n",
    "11. [Feature Engineering](#11-features)\n",
    "12. [Model Implementation](#12-model)\n",
    "13. [Training Pipeline](#13-training)\n",
    "14. [Evaluation & Results](#14-evaluation)\n",
    "15. [Conclusion & Roadmap](#15-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART I: THEORETICAL FOUNDATION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-motivation\"></a>\n",
    "## 1. Motivation & Current System Limitations\n",
    "\n",
    "### 1.1 Current Architecture\n",
    "\n",
    "Our existing system uses:\n",
    "- **Separate LSTM models** per asset for return prediction\n",
    "- **Static correlation matrix** (computed once from historical data)\n",
    "- **Mean-Variance Optimization** with predicted returns\n",
    "\n",
    "### 1.2 Key Limitations\n",
    "\n",
    "| Limitation | Impact | Proposed Solution |\n",
    "|------------|--------|-------------------|\n",
    "| **Single-scale features** | Misses multi-timeframe patterns | TCN with dilated convolutions |\n",
    "| **Static correlations** | Fails during regime changes | Dynamic GNN updates |\n",
    "| **No uncertainty quantification** | Overconfident predictions | Gaussian prediction head |\n",
    "| **Single objective** | Trading vs prediction conflict | Multi-head architecture |\n",
    "| **Sequential training only** | Suboptimal convergence | Curriculum learning |\n",
    "\n",
    "### 1.3 Why This Architecture?\n",
    "\n",
    "The proposed TCN-GNN-LSTM architecture addresses all limitations:\n",
    "\n",
    "```\n",
    "Current System:                    Proposed System:\n",
    "+-----------+                      +------------------+\n",
    "| LSTM only | Single-scale         | TCN              | Multi-scale\n",
    "+-----------+                      | (1m, 5m, 1h, 1d) | temporal features\n",
    "     |                             +------------------+\n",
    "     v                                     |\n",
    "+-----------+                              v\n",
    "| Static    | Fixed correlations   +------------------+\n",
    "| Corr Mat  |                      | GNN              | Dynamic, learned\n",
    "+-----------+                      | (attention-based)| relationships\n",
    "     |                             +------------------+\n",
    "     v                                     |\n",
    "+-----------+                              v\n",
    "| MVO       | Point estimate only  +------------------+\n",
    "| Optimizer |                      | LSTM + Multi-Head| Memory + \n",
    "+-----------+                      | (3 outputs)      | uncertainty\n",
    "                                   +------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2-architecture\"></a>\n",
    "## 2. Architecture Overview\n",
    "\n",
    "### 2.1 High-Level Data Flow\n",
    "\n",
    "```\n",
    "                    RAW MARKET DATA\n",
    "                    (OHLCV per asset)\n",
    "                         |\n",
    "                         v\n",
    "    +--------------------------------------------+\n",
    "    |              TCN FEATURE EXTRACTOR         |\n",
    "    |                                            |\n",
    "    |  Dilated Convolutions capture patterns at  |\n",
    "    |  multiple time scales simultaneously       |\n",
    "    |                                            |\n",
    "    |  +--------+  +--------+  +--------+        |\n",
    "    |  |Dilation|  |Dilation|  |Dilation|        |\n",
    "    |  | d=1    |  | d=2    |  | d=4    |  ...   |\n",
    "    |  | 1-day  |  | 2-day  |  | 4-day  |        |\n",
    "    |  +--------+  +--------+  +--------+        |\n",
    "    |       \\          |           /             |\n",
    "    |        \\         |          /              |\n",
    "    |         v        v         v               |\n",
    "    |       [Multi-Scale Feature Vectors]        |\n",
    "    +--------------------------------------------+\n",
    "                         |\n",
    "                         v\n",
    "    +--------------------------------------------+\n",
    "    |           GRAPH NEURAL NETWORK             |\n",
    "    |                                            |\n",
    "    |  Models assets as nodes in a graph         |\n",
    "    |  Learns dynamic relationships via attention|\n",
    "    |                                            |\n",
    "    |    BTC ------- ETH                         |\n",
    "    |     |  \\     / |                           |\n",
    "    |     |   \\   /  |    Edge weights learned   |\n",
    "    |     |    \\ /   |    dynamically per batch  |\n",
    "    |     |     X    |                           |\n",
    "    |     |    / \\   |                           |\n",
    "    |     |   /   \\  |                           |\n",
    "    |    SOL ------ BNB                          |\n",
    "    |                                            |\n",
    "    |       [Cross-Asset Enriched Features]      |\n",
    "    +--------------------------------------------+\n",
    "                         |\n",
    "                         v\n",
    "    +--------------------------------------------+\n",
    "    |              LSTM PROCESSOR                |\n",
    "    |                                            |\n",
    "    |  Bidirectional LSTM with temporal attention|\n",
    "    |  Captures sequential dependencies and      |\n",
    "    |  maintains memory of important events      |\n",
    "    |                                            |\n",
    "    |    h(t-2) --> h(t-1) --> h(t) --> ...     |\n",
    "    |                                            |\n",
    "    |       [Temporal Context + Memory]          |\n",
    "    +--------------------------------------------+\n",
    "                         |\n",
    "           +-------------+-------------+\n",
    "           |             |             |\n",
    "           v             v             v\n",
    "    +-----------+  +-----------+  +-----------+\n",
    "    |  TRADING  |  | PREDICTION|  |   VALUE   |\n",
    "    |   HEAD    |  |   HEAD    |  |   HEAD    |\n",
    "    |           |  |           |  |           |\n",
    "    | Outputs   |  | Outputs   |  | Outputs   |\n",
    "    | portfolio |  | Gaussian  |  | expected  |\n",
    "    | weights   |  | mu, sigma |  | return V  |\n",
    "    | (softmax) |  | per asset |  | (scalar)  |\n",
    "    +-----------+  +-----------+  +-----------+\n",
    "           |             |             |\n",
    "           v             v             v\n",
    "    [Portfolio     [Uncertainty-  [Actor-Critic\n",
    "     Allocation]    Aware Est.]    RL Training]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Shapes Throughout the Network\n",
    "\n",
    "Understanding the data flow through each component:\n",
    "\n",
    "| Stage | Shape | Description |\n",
    "|-------|-------|-------------|\n",
    "| **Input** | `(B, T, N, F)` | Batch, Time steps, N assets, F features |\n",
    "| **Post-TCN** | `(B, T, N, D)` | D = TCN hidden dimension (e.g., 64) |\n",
    "| **Post-GNN** | `(B, T, N, D)` | Same shape, enriched with cross-asset info |\n",
    "| **Post-LSTM** | `(B, N, H)` | H = LSTM hidden (pooled over time) |\n",
    "| **Trading Head** | `(B, N)` | Portfolio weights (sum to 1 via softmax) |\n",
    "| **Prediction Head** | `(B, N, 2)` | Mean (Î¼) and std (Ïƒ) for each asset |\n",
    "| **Value Head** | `(B, 1)` | Scalar value estimate for RL |\n",
    "\n",
    "**Example dimensions:**\n",
    "- `B` = 32 (batch size)\n",
    "- `T` = 60 (60 days of history)\n",
    "- `N` = 5 (5 crypto assets)\n",
    "- `F` = 50 (50 input features per asset)\n",
    "- `D` = 64 (TCN/GNN hidden dimension)\n",
    "- `H` = 128 (LSTM hidden Ã— 2 for bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3-components\"></a>\n",
    "## 3. Component Deep Dive\n",
    "\n",
    "### 3.1 Temporal Convolutional Network (TCN)\n",
    "\n",
    "#### Why TCN Instead of Just LSTM?\n",
    "\n",
    "Traditional LSTMs process data **sequentially**, which:\n",
    "- Is slow (no parallelization possible)\n",
    "- Has fixed receptive field\n",
    "- Struggles with very long sequences (vanishing gradients)\n",
    "\n",
    "**TCN solves this** using **dilated causal convolutions**:\n",
    "\n",
    "#### How Dilated Convolutions Work\n",
    "\n",
    "```\n",
    "Standard Convolution (d=1):      Dilated Convolution (d=2):     Dilated Convolution (d=4):\n",
    "                                                   \n",
    "Output:   O O O O O O O O        Output:   O O O O O O O O      Output:   O O O O O O O O\n",
    "          |\\|\\|\\|\\|\\|\\|\\|                  | X | X | X | X                |   X   |   X   |\n",
    "          | \\| \\| \\| \\| \\|                 |/ \\|/ \\|/ \\|/ \\               |  / \\  |  / \\  |\n",
    "Input:    I I I I I I I I        Input:    I I I I I I I I      Input:    I I I I I I I I\n",
    "                                                   \n",
    "Receptive Field: 2               Receptive Field: 4             Receptive Field: 8\n",
    "Sees: today + yesterday          Sees: 4 days of patterns       Sees: 8 days of patterns\n",
    "```\n",
    "\n",
    "#### Key Insight: Exponential Receptive Field Growth\n",
    "\n",
    "By stacking layers with dilation rates [1, 2, 4, 8, 16], we achieve:\n",
    "- **Receptive field of 32 timesteps** with only 5 layers\n",
    "- **Fully parallel computation** (unlike LSTM)\n",
    "- **Multi-scale pattern capture** (short-term momentum + long-term trends)\n",
    "\n",
    "#### Causal Property (Critical for Trading!)\n",
    "\n",
    "```\n",
    "CAUSAL (correct):               NON-CAUSAL (wrong - future leakage!):\n",
    "                                \n",
    "    Uses only PAST data             Uses FUTURE data (cheating!)\n",
    "         â†“                                    â†“\n",
    "    [..., t-2, t-1, t] â†’ pred      [t-1, t, t+1, ...] â†’ pred\n",
    "                                              â†‘\n",
    "                                        Can't know this!\n",
    "```\n",
    "\n",
    "Our TCN uses `padding='causal'` to ensure predictions only use past information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Graph Neural Network (GNN) for Cross-Asset Correlations\n",
    "\n",
    "#### Why GNN?\n",
    "\n",
    "Financial assets don't exist in isolation. Their relationships:\n",
    "- Are **non-linear** (not captured by simple correlation coefficients)\n",
    "- Are **time-varying** (correlations spike during market crashes)\n",
    "- Have **asymmetric dependencies** (BTC leads, altcoins follow)\n",
    "\n",
    "#### Traditional vs. GNN Approach\n",
    "\n",
    "```\n",
    "Traditional Correlation:           GNN-Based Relationships:\n",
    "                                   \n",
    "     Static Matrix                      Dynamic Graph\n",
    "   +---+---+---+---+                    BTC\n",
    "   |1.0|0.8|0.6|0.4|                   / | \\\n",
    "   +---+---+---+---+               0.9/  |  \\0.7\n",
    "   |0.8|1.0|0.7|0.5|                 /   |   \\\n",
    "   +---+---+---+---+              ETH    |   SOL\n",
    "   |0.6|0.7|1.0|0.6|                 \\   |   /\n",
    "   +---+---+---+---+               0.8\\  |  /0.6\n",
    "   |0.4|0.5|0.6|1.0|                   \\ | /\n",
    "   +---+---+---+---+                    BNB\n",
    "                                   \n",
    "   Computed ONCE from history       Attention weights LEARNED\n",
    "   Never changes                    and UPDATED every timestep\n",
    "   Misses regime changes            Adapts to market conditions\n",
    "```\n",
    "\n",
    "#### How Graph Attention Works\n",
    "\n",
    "For each pair of assets (i, j), we compute an attention score:\n",
    "\n",
    "```\n",
    "1. Project features:     Q_i = W_q * h_i    (query)\n",
    "                         K_j = W_k * h_j    (key)\n",
    "                         V_j = W_v * h_j    (value)\n",
    "\n",
    "2. Compute attention:    attention(i,j) = softmax(Q_i Â· K_j / âˆšd)\n",
    "\n",
    "3. Aggregate:            h'_i = Î£_j attention(i,j) * V_j\n",
    "```\n",
    "\n",
    "**Result:** Each asset's representation is enriched with information from correlated assets, weighted by learned attention.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "We use **4 attention heads** to capture different types of relationships:\n",
    "- Head 1: Short-term momentum correlation\n",
    "- Head 2: Volatility co-movement\n",
    "- Head 3: Sector relationships (DeFi tokens, L1 chains, etc.)\n",
    "- Head 4: Market-cap based hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM Sequential Processor\n",
    "\n",
    "After TCN extracts multi-scale features and GNN enriches them with cross-asset information, we use LSTM to:\n",
    "\n",
    "1. **Capture longer temporal dependencies** beyond TCN's receptive field\n",
    "2. **Maintain memory** of important events (crashes, halvings, etc.)\n",
    "3. **Aggregate information** over time for final prediction\n",
    "\n",
    "#### Bidirectional Processing\n",
    "\n",
    "```\n",
    "Forward LSTM:   h1 â†’ h2 â†’ h3 â†’ h4 â†’ h5 â†’ h6\n",
    "                 â†˜   â†˜   â†˜   â†˜   â†˜   â†˜\n",
    "                      Concatenate\n",
    "                 â†—   â†—   â†—   â†—   â†—   â†—\n",
    "Backward LSTM:  h1 â† h2 â† h3 â† h4 â† h5 â† h6\n",
    "\n",
    "Result: Each position has context from BOTH past and future\n",
    "        (future here means \"later in the sequence\", not actual future)\n",
    "```\n",
    "\n",
    "#### Temporal Attention\n",
    "\n",
    "Not all timesteps are equally important. We learn attention weights:\n",
    "\n",
    "```\n",
    "Timesteps:    [t-59, t-58, ..., t-2, t-1, t]\n",
    "                â†“     â†“          â†“    â†“   â†“\n",
    "Attention:   [0.01, 0.01, ..., 0.15, 0.20, 0.30]\n",
    "                                      â†‘\n",
    "                            Recent data weighted more\n",
    "                            (but not always - depends on patterns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Head Output System\n",
    "\n",
    "The key innovation of this architecture is the **three-headed output**:\n",
    "\n",
    "```\n",
    "                    LSTM Output\n",
    "                  (batch, N, 256)\n",
    "                        |\n",
    "          +-------------+-------------+\n",
    "          |             |             |\n",
    "          v             v             v\n",
    "   +-----------+  +-----------+  +-----------+\n",
    "   |  TRADING  |  | PREDICTION|  |   VALUE   |\n",
    "   |   HEAD    |  |   HEAD    |  |   HEAD    |\n",
    "   +-----------+  +-----------+  +-----------+\n",
    "   |           |  |           |  |           |\n",
    "   | Dense(N)  |  | Dense(2N) |  | Dense(1)  |\n",
    "   | Softmax   |  | Î¼, log(Ïƒ) |  | Linear    |\n",
    "   |           |  |           |  |           |\n",
    "   +-----------+  +-----------+  +-----------+\n",
    "         |             |             |\n",
    "         v             v             v\n",
    "   [w_1,...,w_N] [Î¼_1,Ïƒ_1,...  [V(state)]\n",
    "   Î£w_i = 1       Î¼_N,Ïƒ_N]      scalar\n",
    "```\n",
    "\n",
    "#### Why Three Heads?\n",
    "\n",
    "| Head | Purpose | Training Signal | Usage |\n",
    "|------|---------|-----------------|-------|\n",
    "| **Trading** | Direct portfolio weights | Sharpe ratio (primary loss) | Production decisions |\n",
    "| **Prediction** | Return distribution | Gaussian NLL (auxiliary loss) | Uncertainty estimation |\n",
    "| **Value** | Expected future reward | TD error (optional) | RL fine-tuning |\n",
    "\n",
    "#### The Prediction Head: Why Gaussian?\n",
    "\n",
    "Instead of predicting a single number, we predict a **distribution**:\n",
    "\n",
    "```\n",
    "Traditional:   \"BTC will return +2.5% tomorrow\"\n",
    "                       â†“\n",
    "               Single point estimate\n",
    "               No confidence information\n",
    "\n",
    "Our approach:  \"BTC will return +2.5% Â± 1.2% (68% confidence)\"\n",
    "                       â†“\n",
    "               Mean (Î¼) = +2.5%\n",
    "               Std (Ïƒ) = 1.2%\n",
    "               We know HOW CONFIDENT we are!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"4-math\"></a>\n",
    "## 4. Mathematical Formulation\n",
    "\n",
    "### 4.1 Complete Model Forward Pass\n",
    "\n",
    "Let's define the full forward pass mathematically:\n",
    "\n",
    "**Input:** $X \\in \\mathbb{R}^{B \\times T \\times N \\times F}$ (batch, time, assets, features)\n",
    "\n",
    "**Step 1: TCN Feature Extraction**\n",
    "$$Z_{tcn} = \\text{TCN}(X) \\in \\mathbb{R}^{B \\times T \\times N \\times D}$$\n",
    "\n",
    "**Step 2: Graph Neural Network**\n",
    "$$Z_{gnn}, A = \\text{GNN}(Z_{tcn}) \\in \\mathbb{R}^{B \\times T \\times N \\times D}, \\mathbb{R}^{B \\times T \\times H \\times N \\times N}$$\n",
    "\n",
    "Where $A$ is the attention matrix (interpretable correlations).\n",
    "\n",
    "**Step 3: LSTM Processing**\n",
    "$$H = \\text{LSTM}(Z_{gnn}) \\in \\mathbb{R}^{B \\times N \\times 2D_{lstm}}$$\n",
    "\n",
    "**Step 4: Multi-Head Outputs**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w &= \\text{softmax}(\\text{TradingHead}(\\bar{H})) \\in \\Delta^{N-1} \\text{ (simplex)}\\\\\n",
    "\\mu, \\sigma &= \\text{PredictionHead}(H) \\in \\mathbb{R}^{N} \\times \\mathbb{R}^{N}_{>0} \\\\\n",
    "V &= \\text{ValueHead}(\\bar{H}) \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\bar{H} = \\frac{1}{N}\\sum_i H_i$ is the mean-pooled representation.\n",
    "\n",
    "### 4.2 Dilated Convolution Formula\n",
    "\n",
    "A dilated convolution with dilation rate $d$ is defined as:\n",
    "\n",
    "$$(x *_d k)(t) = \\sum_{i=0}^{K-1} k(i) \\cdot x(t - d \\cdot i)$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input sequence\n",
    "- $k$ is the kernel of size $K$\n",
    "- $d$ is the dilation rate\n",
    "\n",
    "**Receptive Field Calculation:**\n",
    "\n",
    "For TCN with $L$ layers and dilations $[1, 2, 4, ..., 2^{L-1}]$:\n",
    "\n",
    "$$\\text{Receptive Field} = 1 + (K-1) \\cdot \\sum_{i=0}^{L-1} 2^i = 1 + (K-1) \\cdot (2^L - 1)$$\n",
    "\n",
    "For $K=3$ and $L=4$: RF = $1 + 2 \\cdot 15 = 31$ timesteps\n",
    "\n",
    "### 4.3 Graph Attention Formula\n",
    "\n",
    "For nodes $i$ and $j$ at time $t$:\n",
    "\n",
    "**Attention Score:**\n",
    "$$e_{ij}^{(t)} = \\frac{(W_Q h_i^{(t)})^T (W_K h_j^{(t)})}{\\sqrt{d_k}}$$\n",
    "\n",
    "**Normalized Attention:**\n",
    "$$\\alpha_{ij}^{(t)} = \\frac{\\exp(e_{ij}^{(t)})}{\\sum_{k=1}^{N} \\exp(e_{ik}^{(t)})}$$\n",
    "\n",
    "**Updated Features:**\n",
    "$$h_i'^{(t)} = \\sum_{j=1}^{N} \\alpha_{ij}^{(t)} (W_V h_j^{(t)})$$\n",
    "\n",
    "**Key Insight:** Unlike static correlation, $\\alpha_{ij}^{(t)}$ is computed **fresh for each timestep**, capturing time-varying relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"5-training-theory\"></a>\n",
    "## 5. Training Strategy: Curriculum Learning\n",
    "\n",
    "### 5.1 Why Curriculum Learning?\n",
    "\n",
    "Training a complex multi-head model end-to-end from scratch often fails because:\n",
    "- Gradients from different heads **conflict**\n",
    "- The model hasn't learned good **representations** yet\n",
    "- Trading objectives are **noisy** (reward depends on market randomness)\n",
    "\n",
    "**Curriculum Learning** solves this by training in stages, from simple to complex:\n",
    "\n",
    "### 5.2 Three-Stage Training\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  STAGE 1: REPRESENTATION LEARNING                                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Goal:    Learn good features from data                              â•‘\n",
    "â•‘  Train:   Prediction head ONLY (Gaussian NLL loss)                   â•‘\n",
    "â•‘  Freeze:  Trading head, Value head                                   â•‘\n",
    "â•‘  Why:     Supervised learning is stable, provides clean gradients    â•‘\n",
    "â•‘  Loss:    Î»_pred = 1.0, Î»_trading = 0.0, Î»_value = 0.0              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                â†“\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  STAGE 2: TRADING OBJECTIVE FINE-TUNING                              â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Goal:    Optimize for actual trading performance                    â•‘\n",
    "â•‘  Train:   Trading head (Sharpe loss) + Prediction head (regularizer) â•‘\n",
    "â•‘  Freeze:  Value head                                                 â•‘\n",
    "â•‘  Why:     Now that representations are good, fine-tune for trading   â•‘\n",
    "â•‘  Loss:    Î»_pred = 0.1, Î»_trading = 1.0, Î»_value = 0.0              â•‘\n",
    "â•‘  Note:    Prediction head prevents overfitting to noise              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                                â†“\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  STAGE 3: RL ENHANCEMENT (Optional)                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Goal:    Learn from actual trading simulation                       â•‘\n",
    "â•‘  Train:   All heads with RL (PPO/A2C)                                â•‘\n",
    "â•‘  Why:     RL can discover strategies that supervised learning misses â•‘\n",
    "â•‘  Loss:    Î»_pred = 0.05, Î»_trading = 0.5, Î»_value = 0.45            â•‘\n",
    "â•‘  Caution: RL is unstable, use sparingly                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"6-uncertainty-theory\"></a>\n",
    "## 6. Uncertainty-Aware Prediction\n",
    "\n",
    "### 6.1 The Problem with Point Estimates\n",
    "\n",
    "Traditional models output a single prediction:\n",
    "```\n",
    "Model says: \"BTC will return +2.5% tomorrow\"\n",
    "\n",
    "But is it:  +2.5% Â± 0.5%  (HIGH confidence - trade aggressively!)\n",
    "    or:     +2.5% Â± 5.0%  (LOW confidence - maybe don't trade...)\n",
    "\n",
    "Point estimates DON'T TELL US!\n",
    "```\n",
    "\n",
    "### 6.2 Gaussian Output: Mean + Uncertainty\n",
    "\n",
    "Our prediction head outputs TWO values per asset:\n",
    "- **Î¼ (mean)**: Expected return\n",
    "- **Ïƒ (sigma)**: Uncertainty/confidence\n",
    "\n",
    "### 6.3 Uncertainty-Adjusted Portfolio Weights\n",
    "\n",
    "The key insight: **reduce allocation to uncertain predictions**\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  TRADITIONAL APPROACH                                              â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Model Output:        Raw Weights:                                 â•‘\n",
    "â•‘    BTC: +3%             BTC: 30%                                   â•‘\n",
    "â•‘    ETH: +4%      â†’      ETH: 40%                                   â•‘\n",
    "â•‘    SOL: +2%             SOL: 30%                                   â•‘\n",
    "â•‘                                                                    â•‘\n",
    "â•‘  Problem: What if ETH prediction is very uncertain?                â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  OUR UNCERTAINTY-AWARE APPROACH                                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  Model Output:                    Adjusted Weights:                â•‘\n",
    "â•‘    BTC: +3% (Ïƒ=1%)  HIGH conf       BTC: 45%  â†‘                   â•‘\n",
    "â•‘    ETH: +4% (Ïƒ=5%)  LOW conf  â†’     ETH: 25%  â†“ (penalized!)      â•‘\n",
    "â•‘    SOL: +2% (Ïƒ=2%)  MED conf        SOL: 30%                      â•‘\n",
    "â•‘                                                                    â•‘\n",
    "â•‘  Formula: w_adjusted = w_raw Ã— exp(-penalty Ã— Ïƒ)                   â•‘\n",
    "â•‘           then normalize to sum to 1                               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "### 6.4 When NOT to Trade\n",
    "\n",
    "If average confidence is below a threshold, we can **skip trading**:\n",
    "\n",
    "```python\n",
    "if avg_confidence < 0.5:\n",
    "    action = \"HOLD - Model is too uncertain\"\n",
    "else:\n",
    "    action = \"TRADE with uncertainty-adjusted weights\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"7-loss-theory\"></a>\n",
    "## 7. Loss Function Design\n",
    "\n",
    "### 7.1 Multi-Objective Loss\n",
    "\n",
    "The total loss is a weighted combination:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\lambda_1 \\mathcal{L}_{trading} + \\lambda_2 \\mathcal{L}_{prediction} + \\lambda_3 \\mathcal{L}_{value}$$\n",
    "\n",
    "### 7.2 Trading Loss: Negative Sharpe Ratio\n",
    "\n",
    "$$\\mathcal{L}_{trading} = -\\text{Sharpe}(r_p) = -\\frac{\\mathbb{E}[r_p]}{\\text{Std}[r_p]} \\times \\sqrt{252}$$\n",
    "\n",
    "Where $r_p = \\sum_i w_i \\cdot r_i$ is the portfolio return.\n",
    "\n",
    "**Why Sharpe?** We want to maximize **risk-adjusted** returns, not just returns.\n",
    "\n",
    "### 7.3 Prediction Loss: Gaussian Negative Log-Likelihood\n",
    "\n",
    "$$\\mathcal{L}_{prediction} = \\frac{1}{2} \\left[ \\log(\\sigma^2) + \\frac{(y - \\mu)^2}{\\sigma^2} \\right]$$\n",
    "\n",
    "**Key Properties:**\n",
    "```\n",
    "If prediction is CORRECT (y â‰ˆ Î¼):\n",
    "  - Small Ïƒ â†’ Small loss  (rewarded for confidence!)\n",
    "  - Large Ïƒ â†’ Medium loss (unnecessary uncertainty)\n",
    "\n",
    "If prediction is WRONG (y â‰  Î¼):\n",
    "  - Small Ïƒ â†’ LARGE loss  (punished for overconfidence!)\n",
    "  - Large Ïƒ â†’ Medium loss (at least we knew we were uncertain)\n",
    "```\n",
    "\n",
    "This naturally teaches the model to be:\n",
    "- **Confident** when it can be accurate\n",
    "- **Uncertain** when predictions are unreliable\n",
    "\n",
    "### 7.4 Why Prediction Loss as Regularizer?\n",
    "\n",
    "Using prediction loss (with small weight Î»â‚‚=0.1) during trading training:\n",
    "- **Prevents overfitting** to spurious trading patterns\n",
    "- **Forces backbone** to learn generalizable features\n",
    "- Acts like **auxiliary task** in multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART II: IMPLEMENTATION\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8-setup\"></a>\n",
    "## 8. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSTALL DEPENDENCIES (Run this first in Colab)\n",
    "# ============================================\n",
    "# !pip install ccxt ta tensorflow pandas numpy matplotlib seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Data fetching\n",
    "import ccxt\n",
    "\n",
    "# Technical analysis\n",
    "import ta\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Utilities\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TCN-GNN-LSTM HYBRID ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy:      {np.__version__}\")\n",
    "print(f\"Pandas:     {pd.__version__}\")\n",
    "print(f\"CCXT:       {ccxt.__version__}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9-data\"></a>\n",
    "## 9. Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for cryptocurrency OHLCV data.\n",
    "    \n",
    "    Uses CCXT library for unified exchange API access.\n",
    "    Supports chunked fetching for large date ranges.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, exchange_name='binance'):\n",
    "        self.exchange = self._init_exchange(exchange_name)\n",
    "        self.cache = {}\n",
    "        \n",
    "    def _init_exchange(self, name):\n",
    "        \"\"\"Initialize exchange connection.\"\"\"\n",
    "        exchanges = {\n",
    "            'binance': ccxt.binance,\n",
    "            'coinbase': ccxt.coinbase,\n",
    "            'kraken': ccxt.kraken,\n",
    "        }\n",
    "        exchange_class = exchanges.get(name, ccxt.binance)\n",
    "        return exchange_class({'enableRateLimit': True})\n",
    "    \n",
    "    def fetch_ohlcv(self, symbol: str, timeframe: str = '1d', \n",
    "                    limit: int = 365) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data for a symbol.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Trading pair (e.g., 'BTC/USDT')\n",
    "            timeframe: Candle timeframe ('1d', '4h', '1h')\n",
    "            limit: Number of candles to fetch\n",
    "        \"\"\"\n",
    "        cache_key = f\"{symbol}_{timeframe}_{limit}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            data = self.exchange.fetch_ohlcv(symbol, timeframe, limit=limit)\n",
    "            df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            self.cache[cache_key] = df\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {symbol}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def fetch_multi_asset(self, symbols: List[str], timeframe: str = '1d',\n",
    "                          limit: int = 365) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Fetch data for multiple assets.\"\"\"\n",
    "        data = {}\n",
    "        for symbol in symbols:\n",
    "            print(f\"  Fetching {symbol}...\", end=\" \")\n",
    "            df = self.fetch_ohlcv(symbol, timeframe, limit)\n",
    "            if not df.empty:\n",
    "                data[symbol] = df\n",
    "                print(f\"âœ“ {len(df)} candles\")\n",
    "            else:\n",
    "                print(\"âœ— Failed\")\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        return data\n",
    "\n",
    "\n",
    "# Initialize loader\n",
    "loader = CryptoDataLoader('binance')\n",
    "print(f\"\\nExchange: {loader.exchange.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PORTFOLIO CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "PORTFOLIO_ASSETS = [\n",
    "    \"BTC/USDT\",   # Bitcoin\n",
    "    \"ETH/USDT\",   # Ethereum\n",
    "    \"BNB/USDT\",   # Binance Coin\n",
    "    \"SOL/USDT\",   # Solana\n",
    "    \"XRP/USDT\",   # Ripple\n",
    "]\n",
    "\n",
    "TIMEFRAME = '1d'\n",
    "LOOKBACK_DAYS = 365\n",
    "\n",
    "print(f\"Assets: {len(PORTFOLIO_ASSETS)}\")\n",
    "print(f\"Timeframe: {TIMEFRAME}\")\n",
    "print(f\"Lookback: {LOOKBACK_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FETCH DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFetching OHLCV data...\")\n",
    "raw_data = loader.fetch_multi_asset(PORTFOLIO_ASSETS, TIMEFRAME, LOOKBACK_DAYS)\n",
    "print(f\"\\nâœ“ Loaded {len(raw_data)} assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10-eda\"></a>\n",
    "## 10. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRICE VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "colors = ['#F7931A', '#627EEA', '#F3BA2F', '#00FFA3', '#23292F']\n",
    "\n",
    "for idx, (symbol, df) in enumerate(raw_data.items()):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    ax = axes[idx]\n",
    "    ax.plot(df.index, df['close'], color=colors[idx], linewidth=1.5)\n",
    "    ax.fill_between(df.index, df['low'], df['high'], alpha=0.2, color=colors[idx])\n",
    "    ax.set_title(f\"{symbol.replace('/USDT', '')} Price\", fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[5].set_visible(False)\n",
    "plt.suptitle('Asset Price History', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRELATION ANALYSIS (Key insight for GNN!)\n",
    "# ============================================\n",
    "\n",
    "# Calculate returns\n",
    "returns_data = {s.replace('/USDT', ''): df['close'].pct_change().dropna() \n",
    "                for s, df in raw_data.items()}\n",
    "returns_df = pd.DataFrame(returns_data)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = returns_df.corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Static correlation\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            center=0, ax=axes[0], vmin=-1, vmax=1)\n",
    "axes[0].set_title('Static Correlation Matrix', fontweight='bold')\n",
    "\n",
    "# Rolling correlation (shows time-varying nature)\n",
    "for col in returns_df.columns[1:]:\n",
    "    rolling_corr = returns_df['BTC'].rolling(30).corr(returns_df[col])\n",
    "    axes[1].plot(rolling_corr, label=f'BTC-{col}')\n",
    "\n",
    "axes[1].axhline(y=0, color='white', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('30-Day Rolling Correlation with BTC', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(-1, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Key Insight: Correlations vary over time!\")\n",
    "print(\"   â†’ This is why we need a GNN with dynamic edge weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11-features\"></a>\n",
    "## 11. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add comprehensive technical indicators.\n",
    "    \n",
    "    Categories:\n",
    "    - Trend (MA, EMA, MACD)\n",
    "    - Momentum (RSI, Stochastic)\n",
    "    - Volatility (Bollinger, ATR)\n",
    "    - Volume (OBV, MFI)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Moving Averages\n",
    "    for period in [7, 14, 21, 50]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(window=period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(df['close'])\n",
    "    df['macd'] = macd.macd()\n",
    "    df['macd_signal'] = macd.macd_signal()\n",
    "    df['macd_diff'] = macd.macd_diff()\n",
    "    \n",
    "    # RSI\n",
    "    for period in [7, 14, 21]:\n",
    "        df[f'rsi_{period}'] = ta.momentum.RSIIndicator(df['close'], window=period).rsi()\n",
    "    \n",
    "    # Stochastic\n",
    "    stoch = ta.momentum.StochasticOscillator(df['high'], df['low'], df['close'])\n",
    "    df['stoch_k'] = stoch.stoch()\n",
    "    df['stoch_d'] = stoch.stoch_signal()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    bb = ta.volatility.BollingerBands(df['close'])\n",
    "    df['bb_high'] = bb.bollinger_hband()\n",
    "    df['bb_low'] = bb.bollinger_lband()\n",
    "    df['bb_width'] = bb.bollinger_wband()\n",
    "    \n",
    "    # ATR\n",
    "    df['atr_14'] = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close']).average_true_range()\n",
    "    \n",
    "    # Volume indicators\n",
    "    df['obv'] = ta.volume.OnBalanceVolumeIndicator(df['close'], df['volume']).on_balance_volume()\n",
    "    df['mfi'] = ta.volume.MFIIndicator(df['high'], df['low'], df['close'], df['volume']).money_flow_index()\n",
    "    \n",
    "    # Returns\n",
    "    for period in [1, 3, 5, 10]:\n",
    "        df[f'return_{period}d'] = df['close'].pct_change(period)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply to all assets\n",
    "print(\"\\nApplying technical indicators...\")\n",
    "featured_data = {}\n",
    "for symbol, df in raw_data.items():\n",
    "    featured_data[symbol] = add_technical_indicators(df)\n",
    "    print(f\"  {symbol}: {len(featured_data[symbol].columns)} features\")\n",
    "\n",
    "print(f\"\\nâœ“ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12-model\"></a>\n",
    "## 12. Model Implementation\n",
    "\n",
    "### 12.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Preprocess multi-asset data for the model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def prepare_data(self, data: Dict[str, pd.DataFrame], \n",
    "                     target_col: str = 'return_1d') -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"Prepare multi-asset data arrays.\"\"\"\n",
    "        # Find common dates\n",
    "        common_index = None\n",
    "        for df in data.values():\n",
    "            idx = df.index\n",
    "            common_index = idx if common_index is None else common_index.intersection(idx)\n",
    "        \n",
    "        asset_names = [s.replace('/USDT', '') for s in data.keys()]\n",
    "        \n",
    "        # Get feature columns\n",
    "        exclude = ['open', 'high', 'low', 'close', 'volume']\n",
    "        feature_cols = [c for c in list(data.values())[0].columns if c not in exclude]\n",
    "        \n",
    "        X_list, y_list = [], []\n",
    "        \n",
    "        for symbol, df in data.items():\n",
    "            df_aligned = df.loc[common_index]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = RobustScaler()\n",
    "            features = df_aligned[feature_cols].values\n",
    "            features = np.nan_to_num(features, nan=0.0)\n",
    "            features = scaler.fit_transform(features)\n",
    "            self.scalers[symbol] = scaler\n",
    "            \n",
    "            X_list.append(features)\n",
    "            y_list.append(df_aligned[target_col].shift(-1).values)\n",
    "        \n",
    "        X = np.stack(X_list, axis=1)  # (time, assets, features)\n",
    "        y = np.stack(y_list, axis=1)  # (time, assets)\n",
    "        y = np.nan_to_num(y, nan=0.0)\n",
    "        \n",
    "        return X[:-1], y[:-1], asset_names\n",
    "    \n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray, \n",
    "                         seq_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Create sequences for temporal models.\"\"\"\n",
    "        X_seq = [X[i-seq_length:i] for i in range(seq_length, len(X))]\n",
    "        y_seq = [y[i] for i in range(seq_length, len(y))]\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "print(\"\\nPreprocessing data...\")\n",
    "preprocessor = DataPreprocessor()\n",
    "X, y, asset_names = preprocessor.prepare_data(featured_data)\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X_seq, y_seq = preprocessor.create_sequences(X, y, SEQ_LENGTH)\n",
    "print(f\"  X_seq shape: {X_seq.shape}\")\n",
    "print(f\"  y_seq shape: {y_seq.shape}\")\n",
    "\n",
    "# Split data\n",
    "n = len(X_seq)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n",
    "X_val, y_val = X_seq[train_end:val_end], y_seq[train_end:val_end]\n",
    "X_test, y_test = X_seq[val_end:], y_seq[val_end:]\n",
    "\n",
    "print(f\"\\n  Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 TCN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Block with:\n",
    "    - Dilated causal convolution\n",
    "    - Residual connection\n",
    "    - Layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size=3, dilation_rate=1, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv1D(filters, kernel_size, dilation_rate=dilation_rate, \n",
    "                                    padding='causal', kernel_initializer='he_normal')\n",
    "        self.conv2 = layers.Conv1D(filters, kernel_size, dilation_rate=dilation_rate,\n",
    "                                    padding='causal', kernel_initializer='he_normal')\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.residual = layers.Conv1D(filters, 1)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = tf.nn.gelu(out)\n",
    "        out = self.dropout1(out, training=training)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = tf.nn.gelu(out)\n",
    "        out = self.dropout2(out, training=training)\n",
    "        \n",
    "        return out + self.residual(x)\n",
    "\n",
    "\n",
    "class TCNFeatureExtractor(layers.Layer):\n",
    "    \"\"\"Multi-scale TCN with exponential dilations.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels=64, num_layers=4, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.blocks = [TCNBlock(channels, dilation_rate=2**i, dropout=dropout)\n",
    "                       for i in range(num_layers)]\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"âœ“ TCN layers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 Graph Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Graph Attention for dynamic cross-asset relationships.\n",
    "    \n",
    "    Computes attention weights between all asset pairs,\n",
    "    allowing the model to learn time-varying correlations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.query = layers.Dense(hidden_dim)\n",
    "        self.key = layers.Dense(hidden_dim)\n",
    "        self.value = layers.Dense(hidden_dim)\n",
    "        self.output_proj = layers.Dense(hidden_dim)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        batch, time, assets, features = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        \n",
    "        # Reshape: (batch * time, assets, features)\n",
    "        x_flat = tf.reshape(x, [-1, assets, self.hidden_dim])\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.query(x_flat)\n",
    "        K = self.key(x_flat)\n",
    "        V = self.value(x_flat)\n",
    "        \n",
    "        # Multi-head reshape\n",
    "        Q = tf.reshape(Q, [-1, assets, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [-1, assets, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [-1, assets, self.num_heads, self.head_dim])\n",
    "        \n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        \n",
    "        # Attention\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        attn = self.dropout(attn, training=training)\n",
    "        \n",
    "        out = tf.matmul(attn, V)\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [-1, assets, self.hidden_dim])\n",
    "        \n",
    "        out = self.output_proj(out)\n",
    "        out = self.norm(out + x_flat)\n",
    "        \n",
    "        return tf.reshape(out, [batch, time, assets, self.hidden_dim])\n",
    "\n",
    "\n",
    "print(\"âœ“ Graph Attention layer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 LSTM Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMProcessor(layers.Layer):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM with temporal attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=64, num_layers=2, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm_layers = [layers.Bidirectional(layers.LSTM(hidden_dim, return_sequences=True, dropout=dropout))\n",
    "                            for _ in range(num_layers)]\n",
    "        self.attention = layers.Dense(1, activation='tanh')\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        batch, time, assets, features = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        \n",
    "        # Process each asset\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])  # (batch, assets, time, features)\n",
    "        x = tf.reshape(x, [-1, time, features])\n",
    "        \n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x, training=training)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attn = tf.nn.softmax(self.attention(x), axis=1)\n",
    "        x = tf.reduce_sum(x * attn, axis=1)\n",
    "        \n",
    "        x = tf.reshape(x, [batch, assets, -1])\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "print(\"âœ“ LSTM Processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5 Multi-Head Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadOutput(layers.Layer):\n",
    "    \"\"\"\n",
    "    Three output heads:\n",
    "    1. Trading: Portfolio weights (softmax)\n",
    "    2. Prediction: Gaussian parameters (Î¼, Ïƒ)\n",
    "    3. Value: Scalar value estimate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_assets, hidden_dim=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_assets = num_assets\n",
    "        \n",
    "        # Trading head\n",
    "        self.trading_dense = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.trading_out = layers.Dense(num_assets)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.pred_dense = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.pred_mu = layers.Dense(num_assets)\n",
    "        self.pred_sigma = layers.Dense(num_assets)\n",
    "        \n",
    "        # Value head\n",
    "        self.value_dense = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.value_out = layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        # Global pooling\n",
    "        global_feat = tf.reduce_mean(x, axis=1)\n",
    "        \n",
    "        # Trading\n",
    "        trading = self.trading_dense(global_feat)\n",
    "        weights = tf.nn.softmax(self.trading_out(trading), axis=-1)\n",
    "        \n",
    "        # Prediction\n",
    "        pred = self.pred_dense(x)\n",
    "        mu = tf.reduce_mean(self.pred_mu(pred), axis=-1)\n",
    "        log_sigma = tf.reduce_mean(self.pred_sigma(pred), axis=-1)\n",
    "        log_sigma = tf.clip_by_value(log_sigma, -4.6, 2.3)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        \n",
    "        # Value\n",
    "        value = self.value_dense(global_feat)\n",
    "        value = self.value_out(value)\n",
    "        \n",
    "        return weights, mu, sigma, value\n",
    "\n",
    "\n",
    "print(\"âœ“ Multi-Head Output defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6 Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_GNN_LSTM_Model(Model):\n",
    "    \"\"\"\n",
    "    Complete TCN-GNN-LSTM Hybrid Model.\n",
    "    \n",
    "    Architecture:\n",
    "    Input â†’ TCN â†’ GNN â†’ LSTM â†’ Multi-Head Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_assets, input_features, \n",
    "                 tcn_channels=32, gnn_heads=2, lstm_hidden=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_assets = num_assets\n",
    "        self.input_features = input_features\n",
    "        \n",
    "        self.input_proj = layers.Dense(tcn_channels, activation='relu')\n",
    "        self.tcn = TCNFeatureExtractor(tcn_channels, num_layers=3)\n",
    "        self.gnn = GraphAttentionLayer(tcn_channels, num_heads=gnn_heads)\n",
    "        self.lstm = LSTMProcessor(lstm_hidden, num_layers=1)\n",
    "        self.output_heads = MultiHeadOutput(num_assets, lstm_hidden * 2)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        batch = tf.shape(x)[0]\n",
    "        time = tf.shape(x)[1]\n",
    "        assets = tf.shape(x)[2]\n",
    "        \n",
    "        # TCN per asset\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x = tf.reshape(x, [-1, time, self.input_features])\n",
    "        x = self.input_proj(x)\n",
    "        x = self.tcn(x, training=training)\n",
    "        x = tf.reshape(x, [batch, assets, time, -1])\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        # GNN\n",
    "        x = self.gnn(x, training=training)\n",
    "        \n",
    "        # LSTM\n",
    "        x = self.lstm(x, training=training)\n",
    "        \n",
    "        # Output\n",
    "        return self.output_heads(x, training=training)\n",
    "\n",
    "\n",
    "# Build model\n",
    "num_assets = len(PORTFOLIO_ASSETS)\n",
    "num_features = X_seq.shape[-1]\n",
    "\n",
    "model = TCN_GNN_LSTM_Model(\n",
    "    num_assets=num_assets,\n",
    "    input_features=num_features,\n",
    "    tcn_channels=32,\n",
    "    gnn_heads=2,\n",
    "    lstm_hidden=32\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = tf.random.normal((4, SEQ_LENGTH, num_assets, num_features))\n",
    "w, mu, sigma, v = model(test_input)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL BUILT SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Input:   {test_input.shape}\")\n",
    "print(f\"  Weights: {w.shape}\")\n",
    "print(f\"  Mu:      {mu.shape}\")\n",
    "print(f\"  Sigma:   {sigma.shape}\")\n",
    "print(f\"  Value:   {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"13-training\"></a>\n",
    "## 13. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def sharpe_loss(y_true, weights, epsilon=1e-8):\n",
    "    \"\"\"Negative Sharpe ratio loss.\"\"\"\n",
    "    portfolio_ret = tf.reduce_sum(weights * y_true, axis=-1)\n",
    "    mean_ret = tf.reduce_mean(portfolio_ret)\n",
    "    std_ret = tf.math.reduce_std(portfolio_ret) + epsilon\n",
    "    return -mean_ret / std_ret * tf.math.sqrt(252.0)\n",
    "\n",
    "def gaussian_nll(y_true, mu, sigma, epsilon=1e-8):\n",
    "    \"\"\"Gaussian negative log-likelihood.\"\"\"\n",
    "    sigma = tf.maximum(sigma, epsilon)\n",
    "    nll = 0.5 * (tf.math.log(sigma**2) + (y_true - mu)**2 / sigma**2)\n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "class MultiTaskLoss:\n",
    "    \"\"\"Combined loss with curriculum learning weights.\"\"\"\n",
    "    def __init__(self, lambda_trading=1.0, lambda_pred=0.1):\n",
    "        self.lambda_trading = lambda_trading\n",
    "        self.lambda_pred = lambda_pred\n",
    "        \n",
    "    def __call__(self, y_true, weights, mu, sigma):\n",
    "        l_trading = sharpe_loss(y_true, weights) if self.lambda_trading > 0 else 0\n",
    "        l_pred = gaussian_nll(y_true, mu, sigma) if self.lambda_pred > 0 else 0\n",
    "        return self.lambda_trading * l_trading + self.lambda_pred * l_pred\n",
    "\n",
    "print(\"âœ“ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING\n",
    "# ============================================\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Stage 1: Representation learning\n",
    "print(\"\\n--- Stage 1: Representation Learning ---\")\n",
    "loss_fn = MultiTaskLoss(lambda_trading=0.0, lambda_pred=1.0)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for i in range(0, len(X_train) - 16, 16):\n",
    "        X_batch = tf.constant(X_train[i:i+16], dtype=tf.float32)\n",
    "        y_batch = tf.constant(y_train[i:i+16], dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            w, mu, sigma, v = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, w, mu, sigma)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}: Loss = {np.mean(losses):.4f}\")\n",
    "\n",
    "# Stage 2: Trading fine-tuning\n",
    "print(\"\\n--- Stage 2: Trading Fine-tuning ---\")\n",
    "loss_fn = MultiTaskLoss(lambda_trading=1.0, lambda_pred=0.1)\n",
    "optimizer.learning_rate.assign(5e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for i in range(0, len(X_train) - 16, 16):\n",
    "        X_batch = tf.constant(X_train[i:i+16], dtype=tf.float32)\n",
    "        y_batch = tf.constant(y_train[i:i+16], dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            w, mu, sigma, v = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, w, mu, sigma)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}: Loss = {np.mean(losses):.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"14-evaluation\"></a>\n",
    "## 14. Evaluation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION\n",
    "# ============================================\n",
    "\n",
    "X_test_tensor = tf.constant(X_test, dtype=tf.float32)\n",
    "weights, mu, sigma, value = model(X_test_tensor, training=False)\n",
    "weights = weights.numpy()\n",
    "mu = mu.numpy()\n",
    "sigma = sigma.numpy()\n",
    "\n",
    "# Portfolio returns\n",
    "portfolio_returns = np.sum(weights * y_test, axis=1)\n",
    "\n",
    "# Metrics\n",
    "total_return = (1 + portfolio_returns).prod() - 1\n",
    "sharpe = portfolio_returns.mean() / portfolio_returns.std() * np.sqrt(252)\n",
    "\n",
    "# Max drawdown\n",
    "cumulative = (1 + portfolio_returns).cumprod()\n",
    "running_max = np.maximum.accumulate(cumulative)\n",
    "max_dd = ((cumulative - running_max) / running_max).min()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Total Return:  {total_return*100:.2f}%\")\n",
    "print(f\"  Sharpe Ratio:  {sharpe:.3f}\")\n",
    "print(f\"  Max Drawdown:  {max_dd*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Portfolio allocation\n",
    "ax = axes[0, 0]\n",
    "avg_weights = weights.mean(axis=0)\n",
    "ax.pie(avg_weights, labels=asset_names, autopct='%1.1f%%', colors=colors)\n",
    "ax.set_title('Average Portfolio Allocation', fontweight='bold')\n",
    "\n",
    "# 2. Equity curve\n",
    "ax = axes[0, 1]\n",
    "ax.plot((1 + portfolio_returns).cumprod(), label='TCN-GNN-LSTM', color='cyan', linewidth=2)\n",
    "equal_ret = np.sum(np.ones_like(weights) / num_assets * y_test, axis=1)\n",
    "ax.plot((1 + equal_ret).cumprod(), label='Equal Weight', color='orange', linestyle='--')\n",
    "ax.axhline(y=1, color='white', linestyle=':', alpha=0.3)\n",
    "ax.set_title('Cumulative Returns', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Uncertainty over time\n",
    "ax = axes[1, 0]\n",
    "for i, name in enumerate(asset_names):\n",
    "    ax.plot(sigma[:, i], label=name, alpha=0.7)\n",
    "ax.set_title('Prediction Uncertainty (Ïƒ)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Weights over time\n",
    "ax = axes[1, 1]\n",
    "ax.stackplot(range(len(weights)), weights.T, labels=asset_names, colors=colors, alpha=0.8)\n",
    "ax.set_title('Portfolio Weights Over Time', fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"15-conclusion\"></a>\n",
    "## 15. Conclusion & Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘               TCN-GNN-LSTM ARCHITECTURE SUMMARY                      â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  COMPONENTS IMPLEMENTED:                                             â•‘\n",
    "â•‘  âœ“ TCN Feature Extractor (multi-scale patterns)                      â•‘\n",
    "â•‘  âœ“ Graph Neural Network (dynamic correlations)                       â•‘\n",
    "â•‘  âœ“ LSTM Processor (temporal memory)                                  â•‘\n",
    "â•‘  âœ“ Multi-Head Output (trading + prediction + value)                  â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  KEY INNOVATIONS:                                                    â•‘\n",
    "â•‘  â€¢ Uncertainty quantification via Gaussian head                      â•‘\n",
    "â•‘  â€¢ Curriculum learning (3 stages)                                    â•‘\n",
    "â•‘  â€¢ Dynamic correlation modeling via attention                        â•‘\n",
    "â•‘  â€¢ Multi-task regularization                                         â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  NEXT STEPS:                                                         â•‘\n",
    "â•‘  1. Scale to 20+ assets                                              â•‘\n",
    "â•‘  2. Add Stage 3 (RL enhancement)                                     â•‘\n",
    "â•‘  3. Implement real-time inference                                    â•‘\n",
    "â•‘  4. Production deployment                                            â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
