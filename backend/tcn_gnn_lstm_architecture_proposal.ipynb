{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN-GNN-LSTM Hybrid Architecture: Complete Implementation\n",
    "\n",
    "## A Full End-to-End Deep Learning Pipeline for Multi-Asset Crypto Portfolio Optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Environment Setup & Library Imports**\n",
    "2. **Data Loading Pipeline**\n",
    "   - 2.1 Exchange Configuration\n",
    "   - 2.2 Multi-Asset OHLCV Fetching\n",
    "   - 2.3 Data Quality Checks\n",
    "3. **Exploratory Data Analysis (EDA)**\n",
    "   - 3.1 Price Visualization\n",
    "   - 3.2 Returns Distribution\n",
    "   - 3.3 Correlation Analysis\n",
    "   - 3.4 Volatility Analysis\n",
    "4. **Feature Engineering**\n",
    "   - 4.1 Technical Indicators\n",
    "   - 4.2 Multi-Timeframe Features\n",
    "   - 4.3 Cross-Asset Features\n",
    "5. **Data Preprocessing**\n",
    "   - 5.1 Scaling & Normalization\n",
    "   - 5.2 Sequence Creation\n",
    "   - 5.3 Train/Val/Test Split\n",
    "6. **Model Architecture**\n",
    "   - 6.1 TCN Feature Extractor\n",
    "   - 6.2 Graph Neural Network (GNN)\n",
    "   - 6.3 LSTM Processor\n",
    "   - 6.4 Multi-Head Output\n",
    "   - 6.5 Complete Model Assembly\n",
    "7. **Loss Functions**\n",
    "   - 7.1 Trading Loss (Sharpe-based)\n",
    "   - 7.2 Prediction Loss (Gaussian NLL)\n",
    "   - 7.3 Combined Multi-Task Loss\n",
    "8. **Training Pipeline**\n",
    "   - 8.1 Curriculum Learning Strategy\n",
    "   - 8.2 Training Loop\n",
    "   - 8.3 Validation & Monitoring\n",
    "9. **Evaluation & Backtesting**\n",
    "   - 9.1 Performance Metrics\n",
    "   - 9.2 Uncertainty Analysis\n",
    "   - 9.3 Portfolio Simulation\n",
    "10. **Conclusion & Next Steps**\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** AI Trading Research Team  \n",
    "**Version:** 2.0 (Complete Implementation)  \n",
    "**Last Updated:** February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup & Library Imports\n",
    "\n",
    "First, let's import all necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORE LIBRARIES\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# ============================================\n",
    "# DATA FETCHING\n",
    "# ============================================\n",
    "import ccxt\n",
    "\n",
    "# ============================================\n",
    "# TECHNICAL ANALYSIS\n",
    "# ============================================\n",
    "import ta\n",
    "\n",
    "# ============================================\n",
    "# MACHINE LEARNING\n",
    "# ============================================\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# ============================================\n",
    "# OPTIMIZATION\n",
    "# ============================================\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "# ============================================\n",
    "# UTILITIES\n",
    "# ============================================\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# Print environment info\n",
    "print(\"=\"*70)\n",
    "print(\"TCN-GNN-LSTM HYBRID ARCHITECTURE - COMPLETE IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"NumPy Version:      {np.__version__}\")\n",
    "print(f\"Pandas Version:     {pd.__version__}\")\n",
    "print(f\"CCXT Version:       {ccxt.__version__}\")\n",
    "print(f\"Timestamp:          {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nâœ“ GPU Available: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "else:\n",
    "    print(\"\\nâš  No GPU detected. Training will use CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data Loading Pipeline\n",
    "\n",
    "### 2.1 Exchange Configuration\n",
    "\n",
    "We use CCXT library to fetch data from cryptocurrency exchanges. This provides a unified API for 100+ exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for cryptocurrency OHLCV data from multiple exchanges.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-exchange support (Binance, Coinbase, Kraken, etc.)\n",
    "    - Chunked fetching for large date ranges\n",
    "    - Rate limit handling\n",
    "    - Data caching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, exchange_name='binance'):\n",
    "        \"\"\"\n",
    "        Initialize the data loader.\n",
    "        \n",
    "        Args:\n",
    "            exchange_name: Name of the exchange ('binance', 'coinbase', 'kraken')\n",
    "        \"\"\"\n",
    "        self.exchange_name = exchange_name\n",
    "        self.exchange = self._init_exchange(exchange_name)\n",
    "        self.data_cache = {}\n",
    "        \n",
    "    def _init_exchange(self, name):\n",
    "        \"\"\"Initialize exchange connection.\"\"\"\n",
    "        exchange_map = {\n",
    "            'binance': ccxt.binanceus,\n",
    "            'coinbase': ccxt.coinbase,\n",
    "            'kraken': ccxt.kraken,\n",
    "            'kucoin': ccxt.kucoin,\n",
    "        }\n",
    "        \n",
    "        exchange_class = exchange_map.get(name, ccxt.binanceus)\n",
    "        return exchange_class({\n",
    "            'enableRateLimit': True,\n",
    "            'options': {'defaultType': 'spot'}\n",
    "        })\n",
    "    \n",
    "    def get_timeframe_ms(self, timeframe: str) -> int:\n",
    "        \"\"\"Convert timeframe string to milliseconds.\"\"\"\n",
    "        timeframe_map = {\n",
    "            '1m': 60 * 1000,\n",
    "            '5m': 5 * 60 * 1000,\n",
    "            '15m': 15 * 60 * 1000,\n",
    "            '30m': 30 * 60 * 1000,\n",
    "            '1h': 60 * 60 * 1000,\n",
    "            '4h': 4 * 60 * 60 * 1000,\n",
    "            '1d': 24 * 60 * 60 * 1000,\n",
    "        }\n",
    "        return timeframe_map.get(timeframe, 60 * 60 * 1000)\n",
    "    \n",
    "    def fetch_ohlcv(self, symbol: str, timeframe: str = '1d', \n",
    "                    limit: int = 365, since: datetime = None,\n",
    "                    until: datetime = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data with support for large date ranges.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Trading pair (e.g., 'BTC/USDT')\n",
    "            timeframe: Candle timeframe ('1m', '5m', '1h', '4h', '1d')\n",
    "            limit: Number of candles to fetch (if no date range)\n",
    "            since: Start date for fetching\n",
    "            until: End date for fetching\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with OHLCV data\n",
    "        \"\"\"\n",
    "        cache_key = f\"{symbol}_{timeframe}_{limit}_{since}_{until}\"\n",
    "        if cache_key in self.data_cache:\n",
    "            print(f\"  Using cached data for {symbol}\")\n",
    "            return self.data_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            since_ms = int(since.timestamp() * 1000) if since else None\n",
    "            until_ms = int(until.timestamp() * 1000) if until else int(datetime.now(timezone.utc).timestamp() * 1000)\n",
    "            \n",
    "            # Calculate timeframe in milliseconds\n",
    "            tf_ms = self.get_timeframe_ms(timeframe)\n",
    "            \n",
    "            # Determine how many candles we need\n",
    "            if since_ms and until_ms:\n",
    "                total_needed = int((until_ms - since_ms) / tf_ms) + 1\n",
    "            else:\n",
    "                total_needed = limit\n",
    "            \n",
    "            # Fetch in chunks (max 1000 per request for most exchanges)\n",
    "            all_data = []\n",
    "            current_since = since_ms\n",
    "            max_per_request = 1000\n",
    "            fetched = 0\n",
    "            \n",
    "            while fetched < total_needed:\n",
    "                fetch_limit = min(max_per_request, total_needed - fetched)\n",
    "                \n",
    "                try:\n",
    "                    data = self.exchange.fetch_ohlcv(\n",
    "                        symbol, timeframe, current_since, fetch_limit\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Fetch error: {e}\")\n",
    "                    if all_data:\n",
    "                        break\n",
    "                    raise\n",
    "                \n",
    "                if not data:\n",
    "                    break\n",
    "                \n",
    "                # Filter by end date\n",
    "                if until_ms:\n",
    "                    data = [d for d in data if d[0] <= until_ms]\n",
    "                \n",
    "                if not data:\n",
    "                    break\n",
    "                \n",
    "                all_data.extend(data)\n",
    "                fetched += len(data)\n",
    "                \n",
    "                # Check if we're done\n",
    "                if len(data) < fetch_limit:\n",
    "                    break\n",
    "                \n",
    "                if until_ms and data[-1][0] >= until_ms:\n",
    "                    break\n",
    "                \n",
    "                # Move to next chunk\n",
    "                current_since = data[-1][0] + 1\n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "            if not all_data:\n",
    "                print(f\"  Warning: No data fetched for {symbol}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df = df.sort_index()\n",
    "            \n",
    "            # Cache the result\n",
    "            self.data_cache[cache_key] = df\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error fetching {symbol}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def fetch_multi_asset(self, symbols: List[str], timeframe: str = '1d',\n",
    "                          limit: int = 365) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data for multiple assets.\n",
    "        \n",
    "        Args:\n",
    "            symbols: List of trading pairs\n",
    "            timeframe: Candle timeframe\n",
    "            limit: Number of candles per asset\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping symbol -> DataFrame\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        print(f\"\\nFetching data for {len(symbols)} assets...\")\n",
    "        \n",
    "        for i, symbol in enumerate(symbols):\n",
    "            print(f\"  [{i+1}/{len(symbols)}] {symbol}...\", end=\" \")\n",
    "            df = self.fetch_ohlcv(symbol, timeframe, limit)\n",
    "            if not df.empty:\n",
    "                data[symbol] = df\n",
    "                print(f\"âœ“ {len(df)} candles\")\n",
    "            else:\n",
    "                print(\"âœ— Failed\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "# Initialize data loader\n",
    "print(\"Initializing Data Loader...\")\n",
    "data_loader = CryptoDataLoader(exchange_name='binance')\n",
    "print(f\"Exchange: {data_loader.exchange.name}\")\n",
    "print(f\"Rate Limit: {data_loader.exchange.enableRateLimit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Asset OHLCV Fetching\n",
    "\n",
    "Let's define our portfolio assets and fetch historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PORTFOLIO CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Define assets for portfolio optimization\n",
    "PORTFOLIO_ASSETS = [\n",
    "    \"BTC/USDT\",   # Bitcoin - Market leader\n",
    "    \"ETH/USDT\",   # Ethereum - Smart contracts\n",
    "    \"BNB/USDT\",   # Binance Coin - Exchange token\n",
    "    \"SOL/USDT\",   # Solana - High-performance\n",
    "    \"XRP/USDT\",   # Ripple - Payments\n",
    "]\n",
    "\n",
    "# Data configuration\n",
    "TIMEFRAME = '1d'       # Daily candles\n",
    "LOOKBACK_DAYS = 365    # 1 year of data\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAssets ({len(PORTFOLIO_ASSETS)}):\")\n",
    "for i, asset in enumerate(PORTFOLIO_ASSETS, 1):\n",
    "    print(f\"  {i}. {asset}\")\n",
    "print(f\"\\nTimeframe: {TIMEFRAME}\")\n",
    "print(f\"Lookback: {LOOKBACK_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FETCH DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FETCHING OHLCV DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fetch data for all assets\n",
    "raw_data = data_loader.fetch_multi_asset(\n",
    "    symbols=PORTFOLIO_ASSETS,\n",
    "    timeframe=TIMEFRAME,\n",
    "    limit=LOOKBACK_DAYS\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully loaded data for {len(raw_data)} assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA QUALITY REPORT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_report = []\n",
    "\n",
    "for symbol, df in raw_data.items():\n",
    "    report = {\n",
    "        'Symbol': symbol.replace('/USDT', ''),\n",
    "        'Rows': len(df),\n",
    "        'Start': df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Missing': df.isnull().sum().sum(),\n",
    "        'Min Price': f\"${df['close'].min():.2f}\",\n",
    "        'Max Price': f\"${df['close'].max():.2f}\",\n",
    "        'Current': f\"${df['close'].iloc[-1]:.2f}\"\n",
    "    }\n",
    "    quality_report.append(report)\n",
    "\n",
    "quality_df = pd.DataFrame(quality_report)\n",
    "print(\"\\n\", quality_df.to_string(index=False))\n",
    "\n",
    "# Check for common date range\n",
    "all_dates = [set(df.index) for df in raw_data.values()]\n",
    "common_dates = set.intersection(*all_dates)\n",
    "print(f\"\\nâœ“ Common data points across all assets: {len(common_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Price Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRICE VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#F7931A', '#627EEA', '#F3BA2F', '#00FFA3', '#23292F']\n",
    "\n",
    "for idx, (symbol, df) in enumerate(raw_data.items()):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot price\n",
    "    ax.plot(df.index, df['close'], color=colors[idx], linewidth=1.5, label='Close')\n",
    "    ax.fill_between(df.index, df['low'], df['high'], alpha=0.2, color=colors[idx])\n",
    "    \n",
    "    # Add moving averages\n",
    "    ma_20 = df['close'].rolling(window=20).mean()\n",
    "    ma_50 = df['close'].rolling(window=50).mean()\n",
    "    ax.plot(df.index, ma_20, '--', color='white', alpha=0.5, linewidth=1, label='MA20')\n",
    "    ax.plot(df.index, ma_50, '--', color='yellow', alpha=0.5, linewidth=1, label='MA50')\n",
    "    \n",
    "    ax.set_title(f\"{symbol.replace('/USDT', '')} Price\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price (USDT)')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[5].set_visible(False)\n",
    "\n",
    "plt.suptitle('Asset Price History with Moving Averages', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Returns Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CALCULATE RETURNS\n",
    "# ============================================\n",
    "\n",
    "# Calculate daily returns for each asset\n",
    "returns_data = {}\n",
    "for symbol, df in raw_data.items():\n",
    "    returns_data[symbol] = df['close'].pct_change().dropna()\n",
    "\n",
    "# Create returns DataFrame\n",
    "returns_df = pd.DataFrame(returns_data)\n",
    "returns_df.columns = [s.replace('/USDT', '') for s in returns_df.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETURNS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate statistics\n",
    "stats_df = pd.DataFrame({\n",
    "    'Mean (%)': (returns_df.mean() * 100).round(3),\n",
    "    'Std (%)': (returns_df.std() * 100).round(3),\n",
    "    'Min (%)': (returns_df.min() * 100).round(2),\n",
    "    'Max (%)': (returns_df.max() * 100).round(2),\n",
    "    'Skewness': returns_df.skew().round(3),\n",
    "    'Kurtosis': returns_df.kurtosis().round(3),\n",
    "    'Sharpe (Ann.)': ((returns_df.mean() / returns_df.std()) * np.sqrt(365)).round(3)\n",
    "})\n",
    "\n",
    "print(\"\\n\", stats_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RETURNS DISTRIBUTION PLOTS\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(returns_df.columns):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    returns_df[col].hist(bins=50, ax=ax, color=colors[idx], alpha=0.7, density=True)\n",
    "    returns_df[col].plot.kde(ax=ax, color='white', linewidth=2)\n",
    "    \n",
    "    # Add normal distribution for comparison\n",
    "    x = np.linspace(returns_df[col].min(), returns_df[col].max(), 100)\n",
    "    normal = stats.norm.pdf(x, returns_df[col].mean(), returns_df[col].std())\n",
    "    ax.plot(x, normal, '--', color='red', linewidth=1.5, label='Normal')\n",
    "    \n",
    "    ax.axvline(x=0, color='white', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f\"{col} Daily Returns Distribution\", fontweight='bold')\n",
    "    ax.set_xlabel('Return')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "axes[5].set_visible(False)\n",
    "plt.suptitle('Returns Distribution Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlation Analysis\n",
    "\n",
    "**Key Insight for GNN:** This correlation matrix shows why we need a Graph Neural Network - assets are interconnected with time-varying relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = returns_df.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full period correlation\n",
    "ax1 = axes[0]\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdYlGn', center=0, ax=ax1,\n",
    "            vmin=-1, vmax=1, square=True,\n",
    "            linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "ax1.set_title('Full Period Correlation Matrix', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Rolling correlation (BTC vs others) - showing time-varying nature\n",
    "ax2 = axes[1]\n",
    "rolling_window = 30\n",
    "\n",
    "for col in returns_df.columns[1:]:  # Skip BTC itself\n",
    "    rolling_corr = returns_df['BTC'].rolling(window=rolling_window).corr(returns_df[col])\n",
    "    ax2.plot(rolling_corr.index, rolling_corr, label=f'BTC-{col}', linewidth=1.5)\n",
    "\n",
    "ax2.axhline(y=0, color='white', linestyle='--', alpha=0.3)\n",
    "ax2.axhline(y=0.5, color='green', linestyle='--', alpha=0.3)\n",
    "ax2.axhline(y=-0.5, color='red', linestyle='--', alpha=0.3)\n",
    "ax2.set_title(f'{rolling_window}-Day Rolling Correlation with BTC', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Correlation')\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.set_ylim(-1, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key Insight: Rolling correlations vary significantly over time!\")\n",
    "print(\"   â†’ This is why we need a GNN with dynamic edge weights\")\n",
    "print(\"   â†’ Static correlation matrices miss regime changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VOLATILITY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "# Calculate rolling volatility (annualized)\n",
    "volatility_window = 30\n",
    "rolling_vol = returns_df.rolling(window=volatility_window).std() * np.sqrt(365) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for idx, col in enumerate(rolling_vol.columns):\n",
    "    ax.plot(rolling_vol.index, rolling_vol[col], label=col, color=colors[idx], linewidth=1.5)\n",
    "\n",
    "ax.set_title(f'{volatility_window}-Day Rolling Annualized Volatility (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Volatility (%)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.axhline(y=50, color='yellow', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.axhline(y=100, color='red', linestyle='--', alpha=0.5, label='100% threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Volatility Insights:\")\n",
    "print(f\"   - Average volatility: {rolling_vol.mean().mean():.1f}%\")\n",
    "print(f\"   - Max volatility spike: {rolling_vol.max().max():.1f}%\")\n",
    "print(\"   â†’ High volatility clustering suggests GARCH effects\")\n",
    "print(\"   â†’ TCN can capture multi-scale volatility patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1 Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add comprehensive technical indicators to OHLCV data.\n",
    "    \n",
    "    Categories:\n",
    "    - Trend indicators (MA, EMA, MACD)\n",
    "    - Momentum indicators (RSI, Stochastic, Williams %R)\n",
    "    - Volatility indicators (Bollinger Bands, ATR)\n",
    "    - Volume indicators (OBV, MFI, VWAP)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added technical indicators\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ========== TREND INDICATORS ==========\n",
    "    # Simple Moving Averages\n",
    "    for period in [7, 14, 21, 50, 100]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(window=period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # MACD\n",
    "    macd = ta.trend.MACD(df['close'])\n",
    "    df['macd'] = macd.macd()\n",
    "    df['macd_signal'] = macd.macd_signal()\n",
    "    df['macd_diff'] = macd.macd_diff()\n",
    "    \n",
    "    # ADX (Average Directional Index)\n",
    "    adx = ta.trend.ADXIndicator(df['high'], df['low'], df['close'])\n",
    "    df['adx'] = adx.adx()\n",
    "    df['adx_pos'] = adx.adx_pos()\n",
    "    df['adx_neg'] = adx.adx_neg()\n",
    "    \n",
    "    # ========== MOMENTUM INDICATORS ==========\n",
    "    # RSI\n",
    "    for period in [7, 14, 21]:\n",
    "        df[f'rsi_{period}'] = ta.momentum.RSIIndicator(df['close'], window=period).rsi()\n",
    "    \n",
    "    # Stochastic\n",
    "    stoch = ta.momentum.StochasticOscillator(df['high'], df['low'], df['close'])\n",
    "    df['stoch_k'] = stoch.stoch()\n",
    "    df['stoch_d'] = stoch.stoch_signal()\n",
    "    \n",
    "    # Williams %R\n",
    "    df['williams_r'] = ta.momentum.WilliamsRIndicator(df['high'], df['low'], df['close']).williams_r()\n",
    "    \n",
    "    # ROC (Rate of Change)\n",
    "    for period in [5, 10, 20]:\n",
    "        df[f'roc_{period}'] = ta.momentum.ROCIndicator(df['close'], window=period).roc()\n",
    "    \n",
    "    # ========== VOLATILITY INDICATORS ==========\n",
    "    # Bollinger Bands\n",
    "    bb = ta.volatility.BollingerBands(df['close'])\n",
    "    df['bb_high'] = bb.bollinger_hband()\n",
    "    df['bb_low'] = bb.bollinger_lband()\n",
    "    df['bb_mid'] = bb.bollinger_mavg()\n",
    "    df['bb_width'] = bb.bollinger_wband()\n",
    "    df['bb_pband'] = bb.bollinger_pband()\n",
    "    \n",
    "    # ATR (Average True Range)\n",
    "    for period in [7, 14, 21]:\n",
    "        df[f'atr_{period}'] = ta.volatility.AverageTrueRange(df['high'], df['low'], df['close'], window=period).average_true_range()\n",
    "    \n",
    "    # ========== VOLUME INDICATORS ==========\n",
    "    # OBV (On-Balance Volume)\n",
    "    df['obv'] = ta.volume.OnBalanceVolumeIndicator(df['close'], df['volume']).on_balance_volume()\n",
    "    \n",
    "    # MFI (Money Flow Index)\n",
    "    df['mfi'] = ta.volume.MFIIndicator(df['high'], df['low'], df['close'], df['volume']).money_flow_index()\n",
    "    \n",
    "    # Volume SMA\n",
    "    df['volume_sma_20'] = df['volume'].rolling(window=20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    \n",
    "    # ========== PRICE FEATURES ==========\n",
    "    # Returns at different horizons\n",
    "    for period in [1, 3, 5, 10, 20]:\n",
    "        df[f'return_{period}d'] = df['close'].pct_change(period)\n",
    "    \n",
    "    # Price position within range\n",
    "    df['high_low_ratio'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)\n",
    "    \n",
    "    # Gap features\n",
    "    df['gap'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply technical indicators to all assets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "featured_data = {}\n",
    "for symbol, df in raw_data.items():\n",
    "    print(f\"  Processing {symbol}...\", end=\" \")\n",
    "    featured_data[symbol] = add_technical_indicators(df)\n",
    "    print(f\"âœ“ {len(featured_data[symbol].columns)} features\")\n",
    "\n",
    "# Show feature list for one asset\n",
    "sample_features = featured_data[PORTFOLIO_ASSETS[0]].columns.tolist()\n",
    "print(f\"\\nâœ“ Total features per asset: {len(sample_features)}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - OHLCV: 5\")\n",
    "print(f\"  - Moving Averages: 10\")\n",
    "print(f\"  - MACD: 3\")\n",
    "print(f\"  - ADX: 3\")\n",
    "print(f\"  - RSI: 3\")\n",
    "print(f\"  - Stochastic: 2\")\n",
    "print(f\"  - Bollinger: 5\")\n",
    "print(f\"  - ATR: 3\")\n",
    "print(f\"  - Volume: 4\")\n",
    "print(f\"  - Returns/Price: ~10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Preprocessing\n",
    "\n",
    "### 5.1 Scaling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocess multi-asset data for TCN-GNN-LSTM model.\n",
    "    \n",
    "    Features:\n",
    "    - Robust scaling (handles outliers)\n",
    "    - NaN handling\n",
    "    - Sequence creation for temporal models\n",
    "    - Train/val/test splitting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def prepare_multi_asset_data(self, data: Dict[str, pd.DataFrame], \n",
    "                                  target_col: str = 'return_1d') -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Prepare multi-asset data for model training.\n",
    "        \n",
    "        Args:\n",
    "            data: Dictionary of DataFrames (symbol -> df)\n",
    "            target_col: Column to use as target (return)\n",
    "            \n",
    "        Returns:\n",
    "            X: (time, num_assets, features) array\n",
    "            y: (time, num_assets) array of returns\n",
    "            asset_names: List of asset names\n",
    "        \"\"\"\n",
    "        # Find common dates\n",
    "        common_index = None\n",
    "        for df in data.values():\n",
    "            if common_index is None:\n",
    "                common_index = df.index\n",
    "            else:\n",
    "                common_index = common_index.intersection(df.index)\n",
    "        \n",
    "        asset_names = [s.replace('/USDT', '') for s in data.keys()]\n",
    "        num_assets = len(data)\n",
    "        \n",
    "        # Get feature columns (exclude OHLCV base columns for features)\n",
    "        exclude_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        first_df = list(data.values())[0]\n",
    "        feature_cols = [c for c in first_df.columns if c not in exclude_cols]\n",
    "        self.feature_names = feature_cols\n",
    "        num_features = len(feature_cols)\n",
    "        \n",
    "        print(f\"\\n  Common data points: {len(common_index)}\")\n",
    "        print(f\"  Assets: {num_assets}\")\n",
    "        print(f\"  Features per asset: {num_features}\")\n",
    "        \n",
    "        # Create arrays\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for symbol, df in data.items():\n",
    "            df_aligned = df.loc[common_index].copy()\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = RobustScaler()\n",
    "            features = df_aligned[feature_cols].values\n",
    "            \n",
    "            # Handle NaN\n",
    "            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            features = scaler.fit_transform(features)\n",
    "            \n",
    "            self.scalers[symbol] = scaler\n",
    "            X_list.append(features)\n",
    "            \n",
    "            # Target: next day return (shifted by 1)\n",
    "            returns = df_aligned[target_col].shift(-1).values\n",
    "            returns = np.nan_to_num(returns, nan=0.0)\n",
    "            y_list.append(returns)\n",
    "        \n",
    "        # Stack to (time, num_assets, features)\n",
    "        X = np.stack(X_list, axis=1)  # (time, assets, features)\n",
    "        y = np.stack(y_list, axis=1)  # (time, assets)\n",
    "        \n",
    "        # Remove last row (NaN target)\n",
    "        X = X[:-1]\n",
    "        y = y[:-1]\n",
    "        \n",
    "        return X, y, asset_names\n",
    "    \n",
    "    def create_sequences(self, X: np.ndarray, y: np.ndarray, \n",
    "                         seq_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sequences for temporal models.\n",
    "        \n",
    "        Args:\n",
    "            X: (time, num_assets, features) array\n",
    "            y: (time, num_assets) array\n",
    "            seq_length: Length of input sequences\n",
    "            \n",
    "        Returns:\n",
    "            X_seq: (samples, seq_length, num_assets, features)\n",
    "            y_seq: (samples, num_assets)\n",
    "        \"\"\"\n",
    "        X_seq = []\n",
    "        y_seq = []\n",
    "        \n",
    "        for i in range(seq_length, len(X)):\n",
    "            X_seq.append(X[i-seq_length:i])\n",
    "            y_seq.append(y[i])\n",
    "        \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    def train_val_test_split(self, X: np.ndarray, y: np.ndarray,\n",
    "                              train_ratio: float = 0.7,\n",
    "                              val_ratio: float = 0.15) -> Dict:\n",
    "        \"\"\"\n",
    "        Split data chronologically (no shuffling for time series).\n",
    "        \n",
    "        Args:\n",
    "            X, y: Input and target arrays\n",
    "            train_ratio: Proportion for training\n",
    "            val_ratio: Proportion for validation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with train/val/test splits\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        return {\n",
    "            'X_train': X[:train_end],\n",
    "            'y_train': y[:train_end],\n",
    "            'X_val': X[train_end:val_end],\n",
    "            'y_val': y[train_end:val_end],\n",
    "            'X_test': X[val_end:],\n",
    "            'y_test': y[val_end:]\n",
    "        }\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Prepare multi-asset data\n",
    "X, y, asset_names = preprocessor.prepare_multi_asset_data(featured_data)\n",
    "print(f\"\\n  X shape: {X.shape} (time, assets, features)\")\n",
    "print(f\"  y shape: {y.shape} (time, assets)\")\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 60  # 60 days of history\n",
    "X_seq, y_seq = preprocessor.create_sequences(X, y, seq_length=SEQ_LENGTH)\n",
    "print(f\"\\n  X_seq shape: {X_seq.shape} (samples, seq_len, assets, features)\")\n",
    "print(f\"  y_seq shape: {y_seq.shape} (samples, assets)\")\n",
    "\n",
    "# Split data\n",
    "data_splits = preprocessor.train_val_test_split(X_seq, y_seq)\n",
    "print(f\"\\n  Train: {len(data_splits['X_train'])} samples\")\n",
    "print(f\"  Val:   {len(data_splits['X_val'])} samples\")\n",
    "print(f\"  Test:  {len(data_splits['X_test'])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Architecture\n",
    "\n",
    "### 6.1 TCN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network Block.\n",
    "    \n",
    "    Features:\n",
    "    - Dilated causal convolution (no future information leakage)\n",
    "    - Residual connection\n",
    "    - Layer normalization\n",
    "    - GELU activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size=3, dilation_rate=1, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # First conv block\n",
    "        self.conv1 = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding='causal',\n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv2 = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            padding='causal',\n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        \n",
    "        # Residual projection\n",
    "        self.residual_conv = layers.Conv1D(filters, 1)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = tf.nn.gelu(out)\n",
    "        out = self.dropout1(out, training=training)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = tf.nn.gelu(out)\n",
    "        out = self.dropout2(out, training=training)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = self.residual_conv(x)\n",
    "        return out + residual\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'dropout': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class TCNFeatureExtractor(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-scale TCN feature extractor.\n",
    "    \n",
    "    Uses exponentially increasing dilation rates to capture\n",
    "    patterns at multiple time scales (1, 2, 4, 8, 16 days).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_channels=64, kernel_size=3, num_layers=4, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # TCN blocks with increasing dilation\n",
    "        self.tcn_blocks = [\n",
    "            TCNBlock(\n",
    "                filters=num_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation_rate=2**i,\n",
    "                dropout=dropout,\n",
    "                name=f'tcn_block_{i}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Calculate receptive field\n",
    "        self.receptive_field = 1 + (kernel_size - 1) * sum(2**i for i in range(num_layers))\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"Process input through TCN blocks.\"\"\"\n",
    "        for block in self.tcn_blocks:\n",
    "            x = block(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test TCN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TCN FEATURE EXTRACTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tcn = TCNFeatureExtractor(num_channels=64, num_layers=4)\n",
    "test_input = tf.random.normal((32, 60, 50))  # (batch, time, features)\n",
    "test_output = tcn(test_input)\n",
    "\n",
    "print(f\"\\n  Input shape:  {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Receptive field: {tcn.receptive_field} timesteps\")\n",
    "print(f\"  Dilation rates: [1, 2, 4, 8]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Graph Neural Network (GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Graph Attention Network layer for modeling dynamic asset relationships.\n",
    "    \n",
    "    Key features:\n",
    "    - Multi-head attention\n",
    "    - Dynamic edge weights (learned per timestep)\n",
    "    - Residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads=4, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.query = layers.Dense(hidden_dim, kernel_initializer='glorot_uniform')\n",
    "        self.key = layers.Dense(hidden_dim, kernel_initializer='glorot_uniform')\n",
    "        self.value = layers.Dense(hidden_dim, kernel_initializer='glorot_uniform')\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = layers.Dense(hidden_dim)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x, training=False, return_attention=False):\n",
    "        \"\"\"\n",
    "        Process node features through graph attention.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, time, num_assets, features)\n",
    "            \n",
    "        Returns:\n",
    "            Updated features and optionally attention weights\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        time_steps = tf.shape(x)[1]\n",
    "        num_assets = tf.shape(x)[2]\n",
    "        \n",
    "        # Reshape for attention: (batch * time, assets, features)\n",
    "        x_reshaped = tf.reshape(x, [-1, num_assets, self.hidden_dim])\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.query(x_reshaped)\n",
    "        K = self.key(x_reshaped)\n",
    "        V = self.value(x_reshaped)\n",
    "        \n",
    "        # Reshape for multi-head: (batch*time, heads, assets, head_dim)\n",
    "        Q = tf.reshape(Q, [-1, num_assets, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [-1, num_assets, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [-1, num_assets, self.num_heads, self.head_dim])\n",
    "        \n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])  # (B*T, heads, assets, dim)\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights, training=training)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = tf.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        attended = tf.transpose(attended, [0, 2, 1, 3])\n",
    "        attended = tf.reshape(attended, [-1, num_assets, self.hidden_dim])\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = self.output_proj(attended)\n",
    "        output = self.norm(output + x_reshaped)\n",
    "        \n",
    "        # Reshape to original\n",
    "        output = tf.reshape(output, [batch_size, time_steps, num_assets, self.hidden_dim])\n",
    "        \n",
    "        if return_attention:\n",
    "            attention_weights = tf.reshape(attention_weights, \n",
    "                                           [batch_size, time_steps, self.num_heads, num_assets, num_assets])\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "\n",
    "# Test GNN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRAPH ATTENTION NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gnn = GraphAttentionLayer(hidden_dim=64, num_heads=4)\n",
    "test_input = tf.random.normal((32, 60, 5, 64))  # (batch, time, assets, features)\n",
    "test_output, attn = gnn(test_input, return_attention=True)\n",
    "\n",
    "print(f\"\\n  Input shape:      {test_input.shape}\")\n",
    "print(f\"  Output shape:     {test_output.shape}\")\n",
    "print(f\"  Attention shape:  {attn.shape} (batch, time, heads, assets, assets)\")\n",
    "print(f\"\\n  â†’ Attention weights show how each asset relates to others\")\n",
    "print(f\"  â†’ Weights are dynamic (computed per timestep)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 LSTM Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssetLSTMProcessor(layers.Layer):\n",
    "    \"\"\"\n",
    "    LSTM processor for temporal sequence modeling per asset.\n",
    "    \n",
    "    Features:\n",
    "    - Bidirectional LSTM\n",
    "    - Temporal attention for weighting important timesteps\n",
    "    - Processes each asset independently (GNN already handled cross-asset info)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=128, num_layers=2, dropout=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Stacked Bidirectional LSTM\n",
    "        self.lstm_layers = [\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(hidden_dim, return_sequences=True, dropout=dropout),\n",
    "                name=f'bilstm_{i}'\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.attention = layers.Dense(1, activation='tanh')\n",
    "        self.final_norm = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Process temporal sequences.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, time, num_assets, features)\n",
    "            \n",
    "        Returns:\n",
    "            (batch, num_assets, 2*hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        time_steps = tf.shape(x)[1]\n",
    "        num_assets = tf.shape(x)[2]\n",
    "        features = tf.shape(x)[3]\n",
    "        \n",
    "        # Reshape to process each asset: (batch * assets, time, features)\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])  # (batch, assets, time, features)\n",
    "        x = tf.reshape(x, [-1, time_steps, features])\n",
    "        \n",
    "        # Apply LSTMs\n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x, training=training)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attention_scores = self.attention(x)  # (B*N, T, 1)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "        context = tf.reduce_sum(x * attention_weights, axis=1)  # (B*N, 2*hidden)\n",
    "        \n",
    "        # Reshape back: (batch, assets, 2*hidden)\n",
    "        context = tf.reshape(context, [batch_size, num_assets, -1])\n",
    "        context = self.final_norm(context)\n",
    "        \n",
    "        return context\n",
    "\n",
    "\n",
    "# Test LSTM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM PROCESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstm_proc = AssetLSTMProcessor(hidden_dim=64, num_layers=2)\n",
    "test_input = tf.random.normal((32, 60, 5, 64))\n",
    "test_output = lstm_proc(test_input)\n",
    "\n",
    "print(f\"\\n  Input shape:  {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"\\n  â†’ Output is 2x hidden_dim (bidirectional)\")\n",
    "print(f\"  â†’ Temporal attention weights important timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Multi-Head Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadOutput(layers.Layer):\n",
    "    \"\"\"\n",
    "    Three-headed output layer:\n",
    "    \n",
    "    1. Trading Head: Portfolio weights (softmax)\n",
    "    2. Prediction Head: Gaussian parameters (mu, sigma) for uncertainty\n",
    "    3. Value Head: Expected cumulative return (for RL)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_assets, hidden_dim=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_assets = num_assets\n",
    "        \n",
    "        # Trading head\n",
    "        self.trading_hidden = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.trading_output = layers.Dense(num_assets)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.pred_hidden = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.pred_mu = layers.Dense(num_assets)\n",
    "        self.pred_log_sigma = layers.Dense(num_assets)\n",
    "        \n",
    "        # Value head\n",
    "        self.value_hidden = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.value_output = layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Generate multi-head outputs.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, num_assets, features) from LSTM\n",
    "            \n",
    "        Returns:\n",
    "            trading_weights, pred_mu, pred_sigma, value\n",
    "        \"\"\"\n",
    "        # Global pooling for trading and value heads\n",
    "        global_features = tf.reduce_mean(x, axis=1)  # (batch, features)\n",
    "        \n",
    "        # ===== TRADING HEAD =====\n",
    "        trading_h = self.trading_hidden(global_features)\n",
    "        trading_logits = self.trading_output(trading_h)\n",
    "        trading_weights = tf.nn.softmax(trading_logits, axis=-1)\n",
    "        \n",
    "        # ===== PREDICTION HEAD =====\n",
    "        pred_h = self.pred_hidden(x)  # (batch, assets, hidden)\n",
    "        pred_mu = tf.reduce_mean(self.pred_mu(pred_h), axis=-1)  # (batch, assets)\n",
    "        pred_log_sigma = tf.reduce_mean(self.pred_log_sigma(pred_h), axis=-1)\n",
    "        pred_log_sigma = tf.clip_by_value(pred_log_sigma, -4.6, 2.3)  # sigma in [0.01, 10]\n",
    "        pred_sigma = tf.exp(pred_log_sigma)\n",
    "        \n",
    "        # ===== VALUE HEAD =====\n",
    "        value_h = self.value_hidden(global_features)\n",
    "        value = self.value_output(value_h)\n",
    "        \n",
    "        return trading_weights, pred_mu, pred_sigma, value\n",
    "\n",
    "\n",
    "# Test Multi-Head\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-HEAD OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "multi_head = MultiHeadOutput(num_assets=5, hidden_dim=64)\n",
    "test_input = tf.random.normal((32, 5, 128))\n",
    "weights, mu, sigma, value = multi_head(test_input)\n",
    "\n",
    "print(f\"\\n  Input shape:  {test_input.shape}\")\n",
    "print(f\"\\n  Outputs:\")\n",
    "print(f\"    Trading weights: {weights.shape} (sum to 1)\")\n",
    "print(f\"    Pred mean (mu):  {mu.shape}\")\n",
    "print(f\"    Pred std (sigma):{sigma.shape}\")\n",
    "print(f\"    Value estimate:  {value.shape}\")\n",
    "print(f\"\\n  Sample weights: {weights[0].numpy().round(3)}\")\n",
    "print(f\"  Sum of weights: {weights[0].numpy().sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Complete Model Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_GNN_LSTM_Model(Model):\n",
    "    \"\"\"\n",
    "    Complete TCN-GNN-LSTM Hybrid Model for Portfolio Optimization.\n",
    "    \n",
    "    Architecture:\n",
    "    1. TCN: Multi-scale temporal feature extraction\n",
    "    2. GNN: Dynamic cross-asset relationship modeling\n",
    "    3. LSTM: Sequential processing with memory\n",
    "    4. Multi-Head: Trading, Prediction, Value outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_assets,\n",
    "                 input_features,\n",
    "                 tcn_channels=64,\n",
    "                 tcn_layers=4,\n",
    "                 gnn_heads=4,\n",
    "                 lstm_hidden=64,\n",
    "                 lstm_layers=2,\n",
    "                 dropout=0.2,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_assets = num_assets\n",
    "        self.input_features = input_features\n",
    "        \n",
    "        # Input projection (per asset)\n",
    "        self.input_proj = layers.Dense(tcn_channels, activation='relu')\n",
    "        \n",
    "        # TCN Feature Extractor\n",
    "        self.tcn = TCNFeatureExtractor(\n",
    "            num_channels=tcn_channels,\n",
    "            num_layers=tcn_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Graph Attention Network\n",
    "        self.gnn = GraphAttentionLayer(\n",
    "            hidden_dim=tcn_channels,\n",
    "            num_heads=gnn_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # LSTM Processor\n",
    "        self.lstm = AssetLSTMProcessor(\n",
    "            hidden_dim=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Multi-Head Output\n",
    "        self.output_heads = MultiHeadOutput(\n",
    "            num_assets=num_assets,\n",
    "            hidden_dim=lstm_hidden * 2  # Bidirectional\n",
    "        )\n",
    "        \n",
    "    def call(self, x, training=False, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, time, num_assets, features)\n",
    "            \n",
    "        Returns:\n",
    "            trading_weights, pred_mu, pred_sigma, value\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        time_steps = tf.shape(x)[1]\n",
    "        num_assets = tf.shape(x)[2]\n",
    "        \n",
    "        # Project input features\n",
    "        # Reshape to (batch * assets, time, features) for TCN\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])  # (batch, assets, time, features)\n",
    "        x = tf.reshape(x, [-1, time_steps, self.input_features])\n",
    "        x = self.input_proj(x)  # (batch * assets, time, tcn_channels)\n",
    "        \n",
    "        # TCN\n",
    "        x = self.tcn(x, training=training)  # (batch * assets, time, tcn_channels)\n",
    "        \n",
    "        # Reshape for GNN: (batch, time, assets, features)\n",
    "        x = tf.reshape(x, [batch_size, num_assets, time_steps, -1])\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])  # (batch, time, assets, features)\n",
    "        \n",
    "        # GNN\n",
    "        if return_attention:\n",
    "            x, attention = self.gnn(x, training=training, return_attention=True)\n",
    "        else:\n",
    "            x = self.gnn(x, training=training)\n",
    "        \n",
    "        # LSTM\n",
    "        x = self.lstm(x, training=training)  # (batch, assets, 2*lstm_hidden)\n",
    "        \n",
    "        # Multi-Head Output\n",
    "        outputs = self.output_heads(x, training=training)\n",
    "        \n",
    "        if return_attention:\n",
    "            return outputs + (attention,)\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'num_assets': self.num_assets,\n",
    "            'input_features': self.input_features\n",
    "        }\n",
    "\n",
    "\n",
    "# Build and summarize model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE TCN-GNN-LSTM MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get dimensions from data\n",
    "num_assets = len(PORTFOLIO_ASSETS)\n",
    "seq_length = SEQ_LENGTH\n",
    "num_features = X_seq.shape[-1]\n",
    "\n",
    "print(f\"\\n  Configuration:\")\n",
    "print(f\"    - Assets: {num_assets}\")\n",
    "print(f\"    - Sequence length: {seq_length}\")\n",
    "print(f\"    - Features per asset: {num_features}\")\n",
    "\n",
    "# Create model\n",
    "model = TCN_GNN_LSTM_Model(\n",
    "    num_assets=num_assets,\n",
    "    input_features=num_features,\n",
    "    tcn_channels=64,\n",
    "    tcn_layers=4,\n",
    "    gnn_heads=4,\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = tf.random.normal((4, seq_length, num_assets, num_features))\n",
    "weights, mu, sigma, value = model(test_input)\n",
    "\n",
    "print(f\"\\n  Forward pass test:\")\n",
    "print(f\"    Input:  {test_input.shape}\")\n",
    "print(f\"    Weights: {weights.shape}\")\n",
    "print(f\"    Mu:     {mu.shape}\")\n",
    "print(f\"    Sigma:  {sigma.shape}\")\n",
    "print(f\"    Value:  {value.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "model.build(input_shape=(None, seq_length, num_assets, num_features))\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Loss Functions\n",
    "\n",
    "### 7.1 Trading Loss (Sharpe-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio_loss(y_true, weights, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Negative Sharpe Ratio loss for portfolio optimization.\n",
    "    \n",
    "    Sharpe = mean(returns) / std(returns)\n",
    "    \n",
    "    We minimize -Sharpe to maximize actual Sharpe.\n",
    "    \"\"\"\n",
    "    # Portfolio returns: weighted sum of asset returns\n",
    "    portfolio_returns = tf.reduce_sum(weights * y_true, axis=-1)  # (batch,)\n",
    "    \n",
    "    # Mean and std\n",
    "    mean_return = tf.reduce_mean(portfolio_returns)\n",
    "    std_return = tf.math.reduce_std(portfolio_returns) + epsilon\n",
    "    \n",
    "    # Sharpe ratio (annualized for daily data)\n",
    "    sharpe = mean_return / std_return * tf.math.sqrt(252.0)\n",
    "    \n",
    "    return -sharpe  # Negative for minimization\n",
    "\n",
    "\n",
    "print(\"Trading Loss: Negative Sharpe Ratio\")\n",
    "print(\"====================================\")\n",
    "print(\"Formula: L = -Sharpe(portfolio_returns)\")\n",
    "print(\"Goal: Maximize risk-adjusted returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prediction Loss (Gaussian NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll_loss(y_true, mu, sigma, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Gaussian Negative Log-Likelihood loss.\n",
    "    \n",
    "    This loss:\n",
    "    1. Penalizes wrong predictions\n",
    "    2. Rewards confident correct predictions\n",
    "    3. Penalizes overconfident wrong predictions\n",
    "    \n",
    "    NLL = 0.5 * [log(sigma^2) + (y - mu)^2 / sigma^2]\n",
    "    \"\"\"\n",
    "    sigma = tf.maximum(sigma, epsilon)\n",
    "    variance = tf.square(sigma)\n",
    "    squared_error = tf.square(y_true - mu)\n",
    "    \n",
    "    nll = 0.5 * (tf.math.log(variance) + squared_error / variance)\n",
    "    \n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "\n",
    "print(\"Prediction Loss: Gaussian NLL\")\n",
    "print(\"==============================\")\n",
    "print(\"Formula: L = 0.5 * [log(ÏƒÂ²) + (y - Î¼)Â² / ÏƒÂ²]\")\n",
    "print(\"Goal: Learn to predict returns WITH uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Combined Multi-Task Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss:\n",
    "    \"\"\"\n",
    "    Combined loss for multi-head TCN-GNN-LSTM model.\n",
    "    \n",
    "    L_total = Î»1 * L_trading + Î»2 * L_prediction + Î»3 * L_value\n",
    "    \n",
    "    The prediction loss acts as a regularizer to prevent\n",
    "    overfitting on the trading objective.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_trading=1.0, lambda_pred=0.1, lambda_value=0.0):\n",
    "        self.lambda_trading = lambda_trading\n",
    "        self.lambda_pred = lambda_pred\n",
    "        self.lambda_value = lambda_value\n",
    "        \n",
    "    def __call__(self, y_true, weights, mu, sigma, value):\n",
    "        \"\"\"\n",
    "        Compute combined loss.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Actual returns (batch, assets)\n",
    "            weights: Predicted portfolio weights\n",
    "            mu: Predicted mean returns\n",
    "            sigma: Predicted uncertainty\n",
    "            value: Predicted value estimate\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Trading loss\n",
    "        if self.lambda_trading > 0:\n",
    "            losses['trading'] = sharpe_ratio_loss(y_true, weights)\n",
    "        \n",
    "        # Prediction loss\n",
    "        if self.lambda_pred > 0:\n",
    "            losses['prediction'] = gaussian_nll_loss(y_true, mu, sigma)\n",
    "        \n",
    "        # Value loss\n",
    "        if self.lambda_value > 0:\n",
    "            portfolio_return = tf.reduce_sum(weights * y_true, axis=-1, keepdims=True)\n",
    "            losses['value'] = tf.reduce_mean(tf.square(value - portfolio_return))\n",
    "        \n",
    "        # Weighted sum\n",
    "        total = (\n",
    "            self.lambda_trading * losses.get('trading', 0) +\n",
    "            self.lambda_pred * losses.get('prediction', 0) +\n",
    "            self.lambda_value * losses.get('value', 0)\n",
    "        )\n",
    "        \n",
    "        return total, losses\n",
    "\n",
    "\n",
    "print(\"\\nMulti-Task Loss Configuration\")\n",
    "print(\"==============================\")\n",
    "print(\"Stage 1 (Representation): Î»_trading=0.0, Î»_pred=1.0, Î»_value=0.0\")\n",
    "print(\"Stage 2 (Trading):        Î»_trading=1.0, Î»_pred=0.1, Î»_value=0.0\")\n",
    "print(\"Stage 3 (RL):             Î»_trading=0.5, Î»_pred=0.05, Î»_value=0.45\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training Pipeline\n",
    "\n",
    "### 8.1 Curriculum Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumTrainer:\n",
    "    \"\"\"\n",
    "    Implements 3-stage curriculum learning:\n",
    "    \n",
    "    Stage 1: Learn representations via prediction task\n",
    "    Stage 2: Fine-tune for trading with prediction regularizer\n",
    "    Stage 3: (Optional) RL enhancement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        self.model = model\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.stage = 1\n",
    "        self.loss_fn = MultiTaskLoss()\n",
    "        self.history = {'stage': [], 'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "    def set_stage(self, stage):\n",
    "        \"\"\"Configure loss weights for curriculum stage.\"\"\"\n",
    "        self.stage = stage\n",
    "        \n",
    "        if stage == 1:\n",
    "            # Representation learning\n",
    "            self.loss_fn = MultiTaskLoss(lambda_trading=0.0, lambda_pred=1.0, lambda_value=0.0)\n",
    "            print(\"Stage 1: Representation Learning (prediction only)\")\n",
    "        elif stage == 2:\n",
    "            # Trading fine-tuning\n",
    "            self.loss_fn = MultiTaskLoss(lambda_trading=1.0, lambda_pred=0.1, lambda_value=0.0)\n",
    "            print(\"Stage 2: Trading Fine-tuning (trading + prediction regularizer)\")\n",
    "        elif stage == 3:\n",
    "            # RL enhancement\n",
    "            self.loss_fn = MultiTaskLoss(lambda_trading=0.5, lambda_pred=0.05, lambda_value=0.45)\n",
    "            print(\"Stage 3: RL Enhancement (all heads)\")\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            weights, mu, sigma, value = self.model(X, training=True)\n",
    "            total_loss, losses = self.loss_fn(y, weights, mu, sigma, value)\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        # Gradient clipping\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        return total_loss, losses\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(self, X, y):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        weights, mu, sigma, value = self.model(X, training=False)\n",
    "        total_loss, losses = self.loss_fn(y, weights, mu, sigma, value)\n",
    "        return total_loss, losses\n",
    "    \n",
    "    def train_epoch(self, train_data, val_data, batch_size=32):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        X_train, y_train = train_data\n",
    "        X_val, y_val = val_data\n",
    "        \n",
    "        # Training\n",
    "        train_losses = []\n",
    "        num_batches = len(X_train) // batch_size\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = tf.constant(X_train[start:end], dtype=tf.float32)\n",
    "            y_batch = tf.constant(y_train[start:end], dtype=tf.float32)\n",
    "            \n",
    "            loss, _ = self.train_step(X_batch, y_batch)\n",
    "            train_losses.append(loss.numpy())\n",
    "        \n",
    "        # Validation\n",
    "        X_val_tensor = tf.constant(X_val, dtype=tf.float32)\n",
    "        y_val_tensor = tf.constant(y_val, dtype=tf.float32)\n",
    "        val_loss, _ = self.val_step(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        return np.mean(train_losses), val_loss.numpy()\n",
    "    \n",
    "    def fit(self, train_data, val_data, epochs=50, batch_size=32, patience=10):\n",
    "        \"\"\"Full training loop with early stopping.\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, val_loss = self.train_epoch(train_data, val_data, batch_size)\n",
    "            \n",
    "            self.history['stage'].append(self.stage)\n",
    "            self.history['epoch'].append(epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}/{epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CURRICULUM LEARNING TRAINER\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model\n",
    "model = TCN_GNN_LSTM_Model(\n",
    "    num_assets=num_assets,\n",
    "    input_features=num_features,\n",
    "    tcn_channels=32,  # Reduced for faster training\n",
    "    tcn_layers=3,\n",
    "    gnn_heads=2,\n",
    "    lstm_hidden=32,\n",
    "    lstm_layers=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CurriculumTrainer(model, learning_rate=1e-3)\n",
    "\n",
    "# Prepare data tuples\n",
    "train_data = (data_splits['X_train'], data_splits['y_train'])\n",
    "val_data = (data_splits['X_val'], data_splits['y_val'])\n",
    "\n",
    "# STAGE 1: Representation Learning\n",
    "print(\"\\n--- STAGE 1: Representation Learning ---\")\n",
    "trainer.set_stage(1)\n",
    "history_s1 = trainer.fit(train_data, val_data, epochs=20, batch_size=16, patience=5)\n",
    "\n",
    "# STAGE 2: Trading Fine-tuning\n",
    "print(\"\\n--- STAGE 2: Trading Fine-tuning ---\")\n",
    "trainer.set_stage(2)\n",
    "trainer.optimizer.learning_rate.assign(5e-4)  # Lower LR for fine-tuning\n",
    "history_s2 = trainer.fit(train_data, val_data, epochs=20, batch_size=16, patience=5)\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "history_df = pd.DataFrame(trainer.history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stage 1\n",
    "s1_data = history_df[history_df['stage'] == 1]\n",
    "if len(s1_data) > 0:\n",
    "    axes[0].plot(s1_data['epoch'], s1_data['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0].plot(s1_data['epoch'], s1_data['val_loss'], 'r--', label='Val', linewidth=2)\n",
    "    axes[0].set_title('Stage 1: Representation Learning', fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Stage 2\n",
    "s2_data = history_df[history_df['stage'] == 2]\n",
    "if len(s2_data) > 0:\n",
    "    axes[1].plot(range(len(s2_data)), s2_data['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    axes[1].plot(range(len(s2_data)), s2_data['val_loss'], 'r--', label='Val', linewidth=2)\n",
    "    axes[1].set_title('Stage 2: Trading Fine-tuning', fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss (Neg. Sharpe)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Evaluation & Backtesting\n",
    "\n",
    "### 9.1 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions on test set\n",
    "X_test = tf.constant(data_splits['X_test'], dtype=tf.float32)\n",
    "y_test = data_splits['y_test']\n",
    "\n",
    "weights, mu, sigma, value = model(X_test, training=False)\n",
    "weights = weights.numpy()\n",
    "mu = mu.numpy()\n",
    "sigma = sigma.numpy()\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = np.sum(weights * y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "total_return = (1 + portfolio_returns).prod() - 1\n",
    "mean_return = portfolio_returns.mean()\n",
    "std_return = portfolio_returns.std()\n",
    "sharpe = mean_return / std_return * np.sqrt(252) if std_return > 0 else 0\n",
    "\n",
    "# Max drawdown\n",
    "cumulative = (1 + portfolio_returns).cumprod()\n",
    "running_max = np.maximum.accumulate(cumulative)\n",
    "drawdown = (cumulative - running_max) / running_max\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "print(f\"\\n  Test Period Performance:\")\n",
    "print(f\"  ========================\")\n",
    "print(f\"  Total Return:    {total_return*100:.2f}%\")\n",
    "print(f\"  Mean Daily:      {mean_return*100:.4f}%\")\n",
    "print(f\"  Volatility:      {std_return*100:.4f}%\")\n",
    "print(f\"  Sharpe Ratio:    {sharpe:.3f}\")\n",
    "print(f\"  Max Drawdown:    {max_drawdown*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNCERTAINTY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UNCERTAINTY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate confidence (inverse of sigma)\n",
    "confidence = 1 / (1 + sigma)\n",
    "mean_confidence_per_asset = confidence.mean(axis=0)\n",
    "\n",
    "print(f\"\\n  Confidence per Asset:\")\n",
    "for i, name in enumerate(asset_names):\n",
    "    print(f\"    {name}: {mean_confidence_per_asset[i]:.3f}\")\n",
    "\n",
    "# Plot uncertainty over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average sigma over test period\n",
    "ax1 = axes[0]\n",
    "for i, name in enumerate(asset_names):\n",
    "    ax1.plot(sigma[:, i], label=name, alpha=0.7)\n",
    "ax1.set_title('Prediction Uncertainty (Ïƒ) Over Time', fontweight='bold')\n",
    "ax1.set_xlabel('Test Sample')\n",
    "ax1.set_ylabel('Sigma')\n",
    "ax1.legend()\n",
    "\n",
    "# Confidence vs. Prediction Error\n",
    "ax2 = axes[1]\n",
    "prediction_errors = np.abs(y_test - mu)\n",
    "avg_error = prediction_errors.mean(axis=1)\n",
    "avg_confidence = confidence.mean(axis=1)\n",
    "\n",
    "ax2.scatter(avg_confidence, avg_error, alpha=0.5, c='cyan')\n",
    "ax2.set_title('Confidence vs. Prediction Error', fontweight='bold')\n",
    "ax2.set_xlabel('Average Confidence')\n",
    "ax2.set_ylabel('Average Absolute Error')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(avg_confidence, avg_error, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(np.sort(avg_confidence), p(np.sort(avg_confidence)), 'r--', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n  Key Insight: Lower confidence should correlate with higher errors\")\n",
    "print(\"  â†’ This means the model knows when it's uncertain!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Portfolio Allocation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PORTFOLIO ALLOCATION VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average weights\n",
    "ax1 = axes[0]\n",
    "avg_weights = weights.mean(axis=0)\n",
    "colors_pie = ['#F7931A', '#627EEA', '#F3BA2F', '#00FFA3', '#23292F']\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    avg_weights, \n",
    "    labels=asset_names, \n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors_pie,\n",
    "    explode=[0.02]*len(asset_names)\n",
    ")\n",
    "ax1.set_title('Average Portfolio Allocation', fontweight='bold')\n",
    "\n",
    "# Weights over time\n",
    "ax2 = axes[1]\n",
    "ax2.stackplot(range(len(weights)), weights.T, labels=asset_names, colors=colors_pie, alpha=0.8)\n",
    "ax2.set_title('Portfolio Allocation Over Time', fontweight='bold')\n",
    "ax2.set_xlabel('Test Sample')\n",
    "ax2.set_ylabel('Weight')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Equity Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EQUITY CURVE\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Equity curve\n",
    "ax1 = axes[0]\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "\n",
    "# Compare with equal-weight portfolio\n",
    "equal_weights = np.ones((len(y_test), num_assets)) / num_assets\n",
    "equal_returns = np.sum(equal_weights * y_test, axis=1)\n",
    "equal_cumulative = (1 + equal_returns).cumprod()\n",
    "\n",
    "ax1.plot(cumulative_returns, label='TCN-GNN-LSTM', linewidth=2, color='cyan')\n",
    "ax1.plot(equal_cumulative, label='Equal Weight', linewidth=2, color='orange', linestyle='--')\n",
    "ax1.axhline(y=1, color='white', linestyle=':', alpha=0.5)\n",
    "ax1.set_title('Cumulative Returns Comparison', fontweight='bold')\n",
    "ax1.set_xlabel('Test Sample')\n",
    "ax1.set_ylabel('Cumulative Return')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "ax2 = axes[1]\n",
    "ax2.fill_between(range(len(drawdown)), drawdown * 100, 0, color='red', alpha=0.5)\n",
    "ax2.set_title('Portfolio Drawdown', fontweight='bold')\n",
    "ax2.set_xlabel('Test Sample')\n",
    "ax2.set_ylabel('Drawdown (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comparison\n",
    "equal_total = (1 + equal_returns).prod() - 1\n",
    "print(f\"\\n  Strategy Comparison:\")\n",
    "print(f\"  =====================\")\n",
    "print(f\"  TCN-GNN-LSTM:  {total_return*100:.2f}%\")\n",
    "print(f\"  Equal Weight:  {equal_total*100:.2f}%\")\n",
    "print(f\"  Outperformance: {(total_return - equal_total)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Conclusion & Next Steps\n",
    "\n",
    "### 10.1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TCN-GNN-LSTM ARCHITECTURE - IMPLEMENTATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "ARCHITECTURE COMPONENTS:\n",
    "========================\n",
    "1. TCN Feature Extractor\n",
    "   - Multi-scale temporal patterns (dilations: 1, 2, 4, 8)\n",
    "   - Causal convolutions (no future leakage)\n",
    "   - Residual connections for gradient flow\n",
    "\n",
    "2. Graph Neural Network\n",
    "   - Dynamic cross-asset relationships\n",
    "   - Multi-head attention mechanism\n",
    "   - Time-varying correlation modeling\n",
    "\n",
    "3. LSTM Processor\n",
    "   - Bidirectional for richer context\n",
    "   - Temporal attention for important timesteps\n",
    "   - Sequential memory for long-term patterns\n",
    "\n",
    "4. Multi-Head Output\n",
    "   - Trading: Portfolio weights (softmax)\n",
    "   - Prediction: Gaussian (mean + uncertainty)\n",
    "   - Value: Expected return (for RL)\n",
    "\n",
    "KEY INNOVATIONS:\n",
    "================\n",
    "âœ“ Uncertainty quantification via Gaussian head\n",
    "âœ“ Curriculum learning (3 stages)\n",
    "âœ“ Multi-task regularization\n",
    "âœ“ Dynamic correlation modeling\n",
    "\n",
    "NEXT STEPS:\n",
    "===========\n",
    "1. Increase model capacity for production\n",
    "2. Add more assets (20+)\n",
    "3. Implement Stage 3 (RL enhancement)\n",
    "4. Real-time inference pipeline\n",
    "5. Live trading integration\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
